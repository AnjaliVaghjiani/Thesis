{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ2IZnY0spOf",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zBB_GqePIUt",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8Eu1zo5Q2Z54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "-B9Uv4jEPLeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain faiss-cpu sentence-transformers transformers\n"
      ],
      "metadata": {
        "id": "PaVHh0jR-kvv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bQip4YCFKa4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3t4w4p6nK9CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "vectorstore = FAISS.load_local(\n",
        "    \"/content/drive/MyDrive/rag_index\",\n",
        "    embeddings=embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "DganfMFKEXxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data structure and Memory System"
      ],
      "metadata": {
        "id": "LV53MvRSxp2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================================\n",
        "# STEP 1: Data Structures and Memory System\n",
        "# ========================================================================\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Optional\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class UserProfile:\n",
        "    \"\"\"Store user's basic info, targets, and preferences\"\"\"\n",
        "\n",
        "    def __init__(self, user_id: str):\n",
        "        self.user_id = user_id\n",
        "        self.created_date = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "        # Default targets (can be customized per user)\n",
        "        self.targets = {\n",
        "            'daily_steps': 10000,\n",
        "            'weekly_zone_minutes': 150,\n",
        "            'daily_sleep_hours': 8,\n",
        "            'weekly_exercise_sessions': 3,\n",
        "            'daily_calories': 2000,\n",
        "            'daily_whole_grain': 14,  #grams\n",
        "            'daily_refined_grain': 14, # grams\n",
        "            'weekly_legumes': 5,       # grams\n",
        "            'weekly_nuts': 14, #grams\n",
        "            'weekly_red_meat': 7,          # grams\n",
        "            'weekly_white_meat': 14,#grams\n",
        "            'weekly_fruits': 21,       # grams\n",
        "            'weekly_vegetables': 35,   # grams\n",
        "            'weekly_fish': 28,       # grams\n",
        "            'weekly_eggs': 7,    # grams\n",
        "            'weekly_dairy': 56, # 8 pieces\n",
        "            'weekly_sweet': 56 # pices\n",
        "        }\n",
        "\n",
        "        # User preferences and constraints\n",
        "        self.preferences = {\n",
        "            'preferred_exercises': [],\n",
        "            'food_restrictions': [],\n",
        "            'food_allergies': [],\n",
        "            'schedule_constraints': {},\n",
        "            'health_conditions': []\n",
        "        }\n",
        "\n",
        "        # Learning data\n",
        "        self.response_patterns = {\n",
        "            'follows_exercise_recs': 0.5,  # How often they follow exercise advice\n",
        "            'follows_nutrition_recs': 0.5,\n",
        "            'follows_sleep_recs': 0.5,\n",
        "            'preferred_rec_types': [],\n",
        "            'preferred_foods': [],       # Track which food suggestions user likes\n",
        "            'avoided_foods': []\n",
        "        }\n",
        "\n",
        "    def update_targets(self, new_targets: Dict):\n",
        "        \"\"\"Update user targets\"\"\"\n",
        "        self.targets.update(new_targets)\n",
        "\n",
        "    def update_preferences(self, new_prefs: Dict):\n",
        "        \"\"\"Update user preferences\"\"\"\n",
        "        self.preferences.update(new_prefs)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            'user_id': self.user_id,\n",
        "            'created_date': self.created_date,\n",
        "            'targets': self.targets,\n",
        "            'preferences': self.preferences,\n",
        "            'response_patterns': self.response_patterns\n",
        "        }\n",
        "\n",
        "class WeeklyHealthData:\n",
        "    \"\"\"Store one week's health data\"\"\"\n",
        "\n",
        "    def __init__(self, user_id: str, week_start: str, data: Dict):\n",
        "        self.user_id = user_id\n",
        "        self.week_start = week_start\n",
        "        self.week_number = self._calculate_week_number(week_start)\n",
        "\n",
        "        # Raw weekly data\n",
        "        self.total_steps = data.get('total_steps', 0)\n",
        "        self.zone_minutes = data.get('zone_minutes', 0)\n",
        "        self.sleep_hours = data.get('sleep_hours', [])  # Daily values\n",
        "        self.exercise_sessions = data.get('exercise_sessions', [])\n",
        "        self.mood_scores = data.get('mood_scores', [])\n",
        "        self.food_entries = data.get('food_entries', []) # Store as list of food entries\n",
        "\n",
        "        # Calculate food category totals from entries\n",
        "        self.weekly_food_totals = self._calculate_weekly_food_totals()\n",
        "\n",
        "        # Calculated metrics\n",
        "        self.avg_daily_steps = self.total_steps / 7\n",
        "        self.avg_sleep = np.mean(self.sleep_hours) if self.sleep_hours else 0\n",
        "        self.avg_mood = np.mean(self.mood_scores) if self.mood_scores else 0\n",
        "        self.total_calories = sum([entry.get('calories', 0) for entry in self.food_entries])\n",
        "        self.avg_daily_calories = self.total_calories / 7 if self.food_entries else 0\n",
        "\n",
        "        # Food averages (calculated from weekly totals)\n",
        "        self.daily_food_averages = {k: v / 7 for k, v in self.weekly_food_totals.items()}\n",
        "\n",
        "\n",
        "        # Timestamp\n",
        "        self.created_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    def _calculate_week_number(self, week_start: str) -> int:\n",
        "        \"\"\"Calculate week number since user started\"\"\"\n",
        "        week_date = datetime.strptime(week_start, '%Y-%m-%d')\n",
        "        start_of_year = datetime(week_date.year, 1, 1)\n",
        "        return ((week_date - start_of_year).days // 7) + 1\n",
        "\n",
        "    def _calculate_weekly_food_totals(self) -> Dict:\n",
        "        \"\"\"Aggregate food entries by category\"\"\"\n",
        "        totals = {\n",
        "             'whole_grain': 0,  #grams\n",
        "             'refined_grain': 0, # grams\n",
        "             'legumes': 0,       # grams\n",
        "             'nuts': 0, #grams\n",
        "             'red_meat': 0,          # grams\n",
        "             'white_meat': 0,#grams\n",
        "             'fruits': 0,       # grams\n",
        "             'vegetables': 0,   # grams\n",
        "             'fish': 0,       # grams\n",
        "             'eggs': 0,    # grams\n",
        "             'dairy': 0, # 8 pieces\n",
        "             'sweet': 0 # pices\n",
        "        }\n",
        "        for entry in self.food_entries:\n",
        "            category = entry.get('category')\n",
        "            quantity = entry.get('quantity', 0)\n",
        "            if category and category in totals:\n",
        "                totals[category] += quantity\n",
        "        # Prefix keys with 'weekly_' for consistency with targets\n",
        "        return {f'weekly_{k}': v for k, v in totals.items()}\n",
        "\n",
        "\n",
        "    def get_food_summary(self) -> str:\n",
        "        \"\"\"Get a readable summary of food intake\"\"\"\n",
        "        summary = \"Weekly Food Intake:\\n\"\n",
        "        food_categories = {\n",
        "            'dairy': 'Dairy',\n",
        "            'legumes': 'Legumes',\n",
        "            'red_meat': 'Red Meat',\n",
        "            'white_meat': 'White Meat',\n",
        "            'fish': 'Fish',\n",
        "            'eggs': 'Eggs',\n",
        "            'fruits': 'Fruits',\n",
        "            'vegetables': 'Vegetables',\n",
        "            'whole_grain': 'Whole Grain',\n",
        "            'refined_grain': 'Refined Grain',\n",
        "            'nuts': 'Nuts/Seeds',\n",
        "            'sweet': 'Sweets'\n",
        "        }\n",
        "\n",
        "        for category, display_name in food_categories.items():\n",
        "            weekly_total = self.weekly_food_totals.get(f'weekly_{category}', 0)\n",
        "            daily_avg = self.daily_food_averages.get(f'weekly_{category}', 0) # Use weekly total key for average\n",
        "            summary += f\"• {display_name}: {weekly_total:.1f} total ({daily_avg:.1f}/day)\\n\"\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            'user_id': self.user_id,\n",
        "            'week_start': self.week_start,\n",
        "            'week_number': self.week_number,\n",
        "            'total_steps': self.total_steps,\n",
        "            'zone_minutes': self.zone_minutes,\n",
        "            'sleep_hours': self.sleep_hours,\n",
        "            'exercise_sessions': self.exercise_sessions,\n",
        "            'mood_scores': self.mood_scores,\n",
        "            'food_entries': self.food_entries, # Save raw entries\n",
        "            'weekly_food_totals': self.weekly_food_totals, # Save calculated totals\n",
        "            'avg_daily_steps': self.avg_daily_steps,\n",
        "            'avg_sleep': self.avg_sleep,\n",
        "            'avg_mood': self.avg_mood,\n",
        "            'total_calories': self.total_calories,\n",
        "            'avg_daily_calories': self.avg_daily_calories,\n",
        "            'created_at': self.created_at\n",
        "        }\n",
        "\n",
        "class WeeklyRecommendations:\n",
        "    \"\"\"Store recommendations given for a specific week\"\"\"\n",
        "\n",
        "    def __init__(self, user_id: str, week_start: str, recommendations: Dict):\n",
        "        self.user_id = user_id\n",
        "        self.week_start = week_start\n",
        "        self.recommendations = recommendations\n",
        "        self.created_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        # Track user response to recommendations\n",
        "        self.user_feedback = {\n",
        "            'followed_exercise': None,\n",
        "            'followed_nutrition': None,\n",
        "            'followed_sleep': None,\n",
        "            'difficulty_level': None,  # 1-5 scale\n",
        "            'effectiveness': None,     # 1-5 scale\n",
        "            'notes': \"\"\n",
        "        }\n",
        "\n",
        "    def update_feedback(self, feedback: Dict):\n",
        "        \"\"\"Update user feedback on recommendations\"\"\"\n",
        "        self.user_feedback.update(feedback)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            'user_id': self.user_id,\n",
        "            'week_start': self.week_start,\n",
        "            'recommendations': self.recommendations,\n",
        "            'user_feedback': self.user_feedback,\n",
        "            'created_at': self.created_at\n",
        "        }\n",
        "\n",
        "class HealthDataManager:\n",
        "    \"\"\"Manage all user health data and provide persistence\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir: str = \"health_data\"):\n",
        "        self.data_dir = data_dir\n",
        "        self._ensure_directories()\n",
        "\n",
        "    def _ensure_directories(self):\n",
        "        \"\"\"Create necessary directories\"\"\"\n",
        "        os.makedirs(f\"{self.data_dir}/profiles\", exist_ok=True)\n",
        "        os.makedirs(f\"{self.data_dir}/weekly_data\", exist_ok=True)\n",
        "        os.makedirs(f\"{self.data_dir}/recommendations\", exist_ok=True)\n",
        "\n",
        "    def save_user_profile(self, profile: UserProfile):\n",
        "        \"\"\"Save user profile to file\"\"\"\n",
        "        file_path = f\"{self.data_dir}/profiles/{profile.user_id}.json\"\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(profile.to_dict(), f, indent=2)\n",
        "\n",
        "    def load_user_profile(self, user_id: str) -> Optional[UserProfile]:\n",
        "        \"\"\"Load user profile from file\"\"\"\n",
        "        file_path = f\"{self.data_dir}/profiles/{user_id}.json\"\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                profile = UserProfile(user_id)\n",
        "                profile.targets = data['targets']\n",
        "                profile.preferences = data['preferences']\n",
        "                profile.response_patterns = data['response_patterns']\n",
        "                profile.created_date = data['created_date']\n",
        "                return profile\n",
        "        return None\n",
        "\n",
        "    def save_weekly_data(self, weekly_data: WeeklyHealthData):\n",
        "        \"\"\"Save weekly health data\"\"\"\n",
        "        user_dir = f\"{self.data_dir}/weekly_data/{weekly_data.user_id}\"\n",
        "        os.makedirs(user_dir, exist_ok=True)\n",
        "\n",
        "        file_path = f\"{user_dir}/{weekly_data.week_start}.json\"\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(weekly_data.to_dict(), f, indent=2)\n",
        "\n",
        "    def load_user_weekly_data(self, user_id: str, num_weeks: int = 4) -> List[WeeklyHealthData]:\n",
        "        \"\"\"Load recent weekly data for a user\"\"\"\n",
        "        user_dir = f\"{self.data_dir}/weekly_data/{user_id}\"\n",
        "        if not os.path.exists(user_dir):\n",
        "            return []\n",
        "\n",
        "        # Get all weekly data files\n",
        "        files = [f for f in os.listdir(user_dir) if f.endswith('.json')]\n",
        "        files.sort(reverse=True)  # Most recent first\n",
        "\n",
        "        weekly_data_list = []\n",
        "        for file in files[:num_weeks]:\n",
        "            file_path = f\"{user_dir}/{file}\"\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                # Recreate WeeklyHealthData object\n",
        "                weekly_data = WeeklyHealthData(\n",
        "                    data['user_id'],\n",
        "                    data['week_start'],\n",
        "                    data # Pass the whole dictionary to __init__\n",
        "                )\n",
        "                weekly_data_list.append(weekly_data)\n",
        "\n",
        "        return weekly_data_list\n",
        "\n",
        "    def save_recommendations(self, recommendations: WeeklyRecommendations):\n",
        "        \"\"\"Save weekly recommendations\"\"\"\n",
        "        user_dir = f\"{self.data_dir}/recommendations/{recommendations.user_id}\"\n",
        "        os.makedirs(user_dir, exist_ok=True)\n",
        "\n",
        "        file_path = f\"{user_dir}/{recommendations.week_start}.json\"\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(recommendations.to_dict(), f, indent=2)\n",
        "\n",
        "    def load_user_recommendations(self, user_id: str, num_weeks: int = 4) -> List[WeeklyRecommendations]:\n",
        "        \"\"\"Load recent recommendations for a user\"\"\"\n",
        "        user_dir = f\"{self.data_dir}/recommendations/{user_id}\"\n",
        "        if not os.path.exists(user_dir):\n",
        "            return []\n",
        "\n",
        "        files = [f for f in os.listdir(user_dir) if f.endswith('.json')]\n",
        "        files.sort(reverse=True)\n",
        "\n",
        "        rec_list = []\n",
        "        for file in files[:num_weeks]:\n",
        "            file_path = f\"{user_dir}/{file}\"\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                rec = WeeklyRecommendations(\n",
        "                    data['user_id'],\n",
        "                    data['week_start'],\n",
        "                    data['recommendations']\n",
        "                )\n",
        "                rec.user_feedback = data.get('user_feedback', {}) # Handle missing feedback\n",
        "                rec.created_at = data.get('created_at', datetime.now().strftime('%Y-%m-%d %H:%M:%S')) # Handle missing created_at\n",
        "                rec_list.append(rec)\n",
        "\n",
        "        return rec_list\n",
        "\n",
        "    def get_user_progress_summary(self, user_id: str) -> Dict:\n",
        "        \"\"\"Get overall progress summary for a user\"\"\"\n",
        "        weekly_data = self.load_user_weekly_data(user_id, num_weeks=8)\n",
        "\n",
        "        if not weekly_data:\n",
        "            return {\"message\": \"No data available\"}\n",
        "\n",
        "        # Calculate trends\n",
        "        weeks = len(weekly_data)\n",
        "\n",
        "        # Steps trend\n",
        "        steps_trend = [w.avg_daily_steps for w in reversed(weekly_data)]\n",
        "\n",
        "        # Sleep trend\n",
        "        sleep_trend = [w.avg_sleep for w in reversed(weekly_data)]\n",
        "\n",
        "        # Zone minutes trend\n",
        "        zone_trend = [w.zone_minutes for w in reversed(weekly_data)]\n",
        "\n",
        "        # Mood trend\n",
        "        mood_trend = [w.avg_mood for w in reversed(weekly_data)]\n",
        "\n",
        "        return {\n",
        "            'weeks_tracked': weeks,\n",
        "            'steps_trend': steps_trend,\n",
        "            'sleep_trend': sleep_trend,\n",
        "            'zone_trend': zone_trend,\n",
        "            'mood_trend': mood_trend,\n",
        "            'latest_week': weekly_data[0].to_dict() if weekly_data else None\n",
        "        }\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 1 USAGE EXAMPLE\n",
        "# ========================================================================\n",
        "\n",
        "def example_usage():\n",
        "    \"\"\"Example of how to use the data management system\"\"\"\n",
        "\n",
        "    # Initialize data manager\n",
        "    data_manager = HealthDataManager()\n",
        "\n",
        "    # Create or load user profile\n",
        "    user_id = \"user_123\"\n",
        "    profile = data_manager.load_user_profile(user_id)\n",
        "\n",
        "    if not profile:\n",
        "        # Create new user\n",
        "        profile = UserProfile(user_id)\n",
        "        profile.update_targets({\n",
        "            'daily_steps': 12000,  # Custom target\n",
        "            'weekly_zone_minutes': 180\n",
        "        })\n",
        "        profile.update_preferences({\n",
        "            'preferred_exercises': ['running', 'yoga'],\n",
        "            'food_restrictions': ['gluten-free']\n",
        "        })\n",
        "        data_manager.save_user_profile(profile)\n",
        "        print(f\"Created new user profile for {user_id}\")\n",
        "    else:\n",
        "        print(f\"Loaded existing profile for {user_id}\")\n",
        "\n",
        "    # Example weekly data (using food_entries list)\n",
        "    week_data = {\n",
        "        'total_steps': 68000,\n",
        "        'zone_minutes': 140,\n",
        "        'sleep_hours': [7.5, 6.8, 8.2, 7.0, 6.5, 8.5, 7.8],\n",
        "        'exercise_sessions': [\n",
        "            {'type': 'running', 'duration': 30, 'date': '2024-01-15'},\n",
        "            {'type': 'yoga', 'duration': 45, 'date': '2024-01-17'}\n",
        "        ],\n",
        "        'mood_scores': [7, 6, 8, 7, 5, 8, 7],\n",
        "        'food_entries': [ # Use a list of food entries\n",
        "            {'category': 'fruits', 'quantity': 3, 'date': '2024-01-15', 'calories': 200},\n",
        "            {'category': 'vegetables', 'quantity': 4, 'date': '2024-01-15', 'calories': 150},\n",
        "            {'category': 'red_meat', 'quantity': 1, 'date': '2024-01-15', 'calories': 500},\n",
        "            # ... add more food entries for the week\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Save weekly data\n",
        "    weekly_health_data = WeeklyHealthData(user_id, \"2024-01-15\", week_data)\n",
        "    data_manager.save_weekly_data(weekly_health_data)\n",
        "    print(\"Saved weekly health data\")\n",
        "\n",
        "    # Example recommendations\n",
        "    recommendations = {\n",
        "        'exercise': \"Increase zone minutes by adding 2x20min cardio sessions\",\n",
        "        'nutrition': \"Add protein-rich snacks between meals\",\n",
        "        'sleep': \"Maintain current sleep schedule, it's working well\",\n",
        "        'overall': \"Focus on cardiovascular fitness this week\"\n",
        "    }\n",
        "\n",
        "    weekly_recs = WeeklyRecommendations(user_id, \"2024-01-15\", recommendations)\n",
        "    data_manager.save_recommendations(weekly_recs)\n",
        "    print(\"Saved weekly recommendations\")\n",
        "\n",
        "    # Get progress summary\n",
        "    progress = data_manager.get_user_progress_summary(user_id)\n",
        "    print(f\"Progress summary: {progress}\")\n",
        "\n",
        "    return data_manager\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the system\n",
        "    data_manager = example_usage()"
      ],
      "metadata": {
        "id": "E4m6zuEKxpOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analytics Engine"
      ],
      "metadata": {
        "id": "-N61Jyk9yXV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================================\n",
        "# STEP 2: Analytics Engine (Colab Version)\n",
        "# ========================================================================\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "from datetime import datetime\n",
        "import statistics\n",
        "\n",
        "class HealthAnalytics:\n",
        "    \"\"\"Core analytics engine for health data analysis\"\"\"\n",
        "\n",
        "    def __init__(self, data_manager):\n",
        "        self.data_manager = data_manager\n",
        "\n",
        "    def analyze_weekly_performance(self, user_id: str, current_week_data) -> Dict:\n",
        "        \"\"\"Comprehensive analysis of current week vs targets and trends\"\"\"\n",
        "\n",
        "        # Load user profile and historical data\n",
        "        profile = self.data_manager.load_user_profile(user_id)\n",
        "        historical_data = self.data_manager.load_user_weekly_data(user_id, num_weeks=4)\n",
        "\n",
        "        if not profile:\n",
        "            return {\"error\": \"User profile not found\"}\n",
        "\n",
        "        # Current week performance vs targets\n",
        "        performance = self._calculate_target_performance(current_week_data, profile.targets)\n",
        "\n",
        "        # Trend analysis\n",
        "        trends = self._analyze_trends(current_week_data, historical_data)\n",
        "\n",
        "        # Progress analysis\n",
        "        progress = self._analyze_progress(current_week_data, historical_data)\n",
        "\n",
        "        # Priority areas (what needs most attention)\n",
        "        priorities = self._identify_priority_areas(performance, trends)\n",
        "\n",
        "        #Food-specific analysis\n",
        "        food_analysis = self._analyze_food_patterns(current_week_data, profile.targets)\n",
        "\n",
        "        return {\n",
        "            'user_id': user_id,\n",
        "            'week_start': current_week_data.week_start,\n",
        "            'performance': performance,\n",
        "            'trends': trends,\n",
        "            'progress': progress,\n",
        "            'priorities': priorities,\n",
        "            'food_analysis': food_analysis,\n",
        "            'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "\n",
        "    def _calculate_target_performance(self, current_data, targets: Dict) -> Dict:\n",
        "        \"\"\"Calculate performance against targets\"\"\"\n",
        "\n",
        "        performance = {}\n",
        "\n",
        "        # Steps performance\n",
        "        steps_achievement = (current_data.avg_daily_steps / targets['daily_steps']) * 100\n",
        "        performance['steps'] = {\n",
        "            'current': current_data.avg_daily_steps,\n",
        "            'target': targets['daily_steps'],\n",
        "            'achievement_percent': min(steps_achievement, 150),  # Cap at 150%\n",
        "            'status': self._get_performance_status(steps_achievement)\n",
        "        }\n",
        "\n",
        "        # Zone minutes performance\n",
        "        zone_achievement = (current_data.zone_minutes / targets['weekly_zone_minutes']) * 100\n",
        "        performance['zone_minutes'] = {\n",
        "            'current': current_data.zone_minutes,\n",
        "            'target': targets['weekly_zone_minutes'],\n",
        "            'achievement_percent': min(zone_achievement, 150),\n",
        "            'status': self._get_performance_status(zone_achievement)\n",
        "        }\n",
        "\n",
        "        # Sleep performance\n",
        "        sleep_achievement = (current_data.avg_sleep / targets['daily_sleep_hours']) * 100\n",
        "        performance['sleep'] = {\n",
        "            'current': current_data.avg_sleep,\n",
        "            'target': targets['daily_sleep_hours'],\n",
        "            'achievement_percent': min(sleep_achievement, 120),  # Sleep shouldn't be too much over\n",
        "            'status': self._get_performance_status(sleep_achievement)\n",
        "        }\n",
        "\n",
        "        # Exercise sessions\n",
        "        exercise_achievement = (len(current_data.exercise_sessions) / targets['weekly_exercise_sessions']) * 100\n",
        "        performance['exercise_sessions'] = {\n",
        "            'current': len(current_data.exercise_sessions),\n",
        "            'target': targets['weekly_exercise_sessions'],\n",
        "            'achievement_percent': min(exercise_achievement, 150),\n",
        "            'status': self._get_performance_status(exercise_achievement)\n",
        "        }\n",
        "\n",
        "        food_categories = ['dairy', 'legumes', 'meat', 'fruits', 'vegetables', 'grains', 'nuts_seeds', 'water_glasses']\n",
        "\n",
        "        # Food category performance\n",
        "        food_categories = ['dairy', 'legumes', 'meat', 'fruits', 'vegetables', 'grains', 'nuts_seeds', 'water_glasses']\n",
        "\n",
        "        for category in food_categories:\n",
        "            weekly_key = f'weekly_{category}'\n",
        "            if weekly_key in targets and weekly_key in current_data.weekly_food_totals:\n",
        "                current_intake = current_data.weekly_food_totals[weekly_key]\n",
        "                target_intake = targets[weekly_key]\n",
        "\n",
        "                if target_intake > 0:\n",
        "                    achievement = (current_intake / target_intake) * 100\n",
        "                    performance[category] = {\n",
        "                        'current': current_intake,\n",
        "                        'target': target_intake,\n",
        "                        'achievement_percent': min(achievement, 150),\n",
        "                        'status': self._get_food_performance_status(achievement, category)\n",
        "                    }\n",
        "\n",
        "        # Overall performance score (including food categories)\n",
        "        all_scores = []\n",
        "\n",
        "        # Core metrics (weighted more heavily)\n",
        "        core_metrics = ['steps', 'zone_minutes', 'sleep', 'exercise_sessions']\n",
        "        for metric in core_metrics:\n",
        "            if metric in performance:\n",
        "                all_scores.extend([performance[metric]['achievement_percent']] * 2)  # Double weight\n",
        "\n",
        "        # Food metrics\n",
        "        for category in food_categories:\n",
        "            if category in performance:\n",
        "                all_scores.append(performance[category]['achievement_percent'])\n",
        "\n",
        "        performance['overall'] = {\n",
        "            'score': np.mean(all_scores) if all_scores else 0,\n",
        "            'status': self._get_performance_status(np.mean(all_scores) if all_scores else 0)\n",
        "        }\n",
        "\n",
        "        return performance\n",
        "\n",
        "        # Calories (if target is set)\n",
        "        if targets.get('daily_calories'):\n",
        "            calorie_achievement = (current_data.avg_daily_calories / targets['daily_calories']) * 100\n",
        "            performance['calories'] = {\n",
        "                'current': current_data.avg_daily_calories,\n",
        "                'target': targets['daily_calories'],\n",
        "                'achievement_percent': calorie_achievement,\n",
        "                'status': self._get_calorie_status(calorie_achievement)\n",
        "            }\n",
        "\n",
        "        # Overall performance score\n",
        "        scores = [perf['achievement_percent'] for perf in performance.values()]\n",
        "        performance['overall'] = {\n",
        "            'score': np.mean(scores),\n",
        "            'status': self._get_performance_status(np.mean(scores))\n",
        "        }\n",
        "\n",
        "        return performance\n",
        "\n",
        "    def _analyze_food_patterns(self, current_data, targets: Dict) -> Dict:\n",
        "        \"\"\"Analyze food intake patterns and provide insights\"\"\"\n",
        "\n",
        "        food_analysis = {\n",
        "            'balanced_diet_score': 0,\n",
        "            'deficient_categories': [],\n",
        "            'excess_categories': [],\n",
        "            'dietary_insights': []\n",
        "        }\n",
        "\n",
        "        food_categories = ['dairy', 'legumes', 'meat', 'fruits', 'vegetables', 'grains', 'nuts_seeds', 'water_glasses']\n",
        "        category_scores = []\n",
        "\n",
        "        for category in food_categories:\n",
        "            weekly_key = f'weekly_{category}'\n",
        "            if weekly_key in targets and weekly_key in current_data.weekly_food_totals:\n",
        "                current_intake = current_data.weekly_food_totals[weekly_key]\n",
        "                target_intake = targets[weekly_key]\n",
        "\n",
        "                if target_intake > 0:\n",
        "                    achievement_ratio = current_intake / target_intake\n",
        "                    category_scores.append(min(achievement_ratio, 1.5))  # Cap at 150%\n",
        "\n",
        "                    # Identify deficiencies and excesses\n",
        "                    if achievement_ratio < 0.7:  # Less than 70% of target\n",
        "                        food_analysis['deficient_categories'].append({\n",
        "                            'category': category,\n",
        "                            'current': current_intake,\n",
        "                            'target': target_intake,\n",
        "                            'deficit': target_intake - current_intake\n",
        "                        })\n",
        "                    elif achievement_ratio > 1.3:  # More than 130% of target\n",
        "                        food_analysis['excess_categories'].append({\n",
        "                            'category': category,\n",
        "                            'current': current_intake,\n",
        "                            'target': target_intake,\n",
        "                            'excess': current_intake - target_intake\n",
        "                        })\n",
        "\n",
        "        # Calculate balanced diet score\n",
        "        if category_scores:\n",
        "            food_analysis['balanced_diet_score'] = (np.mean(category_scores) * 100)\n",
        "\n",
        "        # Generate dietary insights\n",
        "        if food_analysis['deficient_categories']:\n",
        "            top_deficiency = max(food_analysis['deficient_categories'], key=lambda x: x['deficit'])\n",
        "            food_analysis['dietary_insights'].append(f\"Increase {top_deficiency['category']} intake by {top_deficiency['deficit']} servings/week\")\n",
        "\n",
        "        if food_analysis['excess_categories']:\n",
        "            top_excess = max(food_analysis['excess_categories'], key=lambda x: x['excess'])\n",
        "            food_analysis['dietary_insights'].append(f\"Consider reducing {top_excess['category']} intake\")\n",
        "\n",
        "        # Check for dietary balance\n",
        "        fruit_veg_total = current_data.weekly_food_totals.get('weekly_fruits', 0) + current_data.weekly_food_totals.get('weekly_vegetables', 0)\n",
        "        if fruit_veg_total < 35:  # Less than 5 servings per day\n",
        "            food_analysis['dietary_insights'].append(\"Increase fruit and vegetable intake for better nutrition\")\n",
        "\n",
        "        protein_sources = current_data.weekly_food_totals.get('weekly_meat', 0) + current_data.weekly_food_totals.get('weekly_legumes', 0)\n",
        "        if protein_sources < 10:\n",
        "            food_analysis['dietary_insights'].append(\"Ensure adequate protein intake from various sources\")\n",
        "\n",
        "        return food_analysis\n",
        "\n",
        "    def _get_food_performance_status(self, achievement_percent: float, category: str) -> str:\n",
        "        \"\"\"Get performance status for food categories\"\"\"\n",
        "\n",
        "        # Different thresholds for different food types\n",
        "        if category in ['fruits', 'vegetables', 'water_glasses']:\n",
        "            # For healthy foods, more is generally better\n",
        "            if achievement_percent >= 90:\n",
        "                return \"excellent\"\n",
        "            elif achievement_percent >= 70:\n",
        "                return \"good\"\n",
        "            elif achievement_percent >= 50:\n",
        "                return \"fair\"\n",
        "            else:\n",
        "                return \"needs_improvement\"\n",
        "        else:\n",
        "            # For other foods, balance is key\n",
        "            if 80 <= achievement_percent <= 120:\n",
        "                return \"excellent\"\n",
        "            elif 60 <= achievement_percent <= 140:\n",
        "                return \"good\"\n",
        "            elif 40 <= achievement_percent <= 160:\n",
        "                return \"fair\"\n",
        "            else:\n",
        "                return \"needs_adjustment\"\n",
        "\n",
        "    def _get_performance_status(self, achievement_percent: float) -> str:\n",
        "        \"\"\"Get performance status based on achievement percentage\"\"\"\n",
        "        if achievement_percent >= 100:\n",
        "            return \"excellent\"\n",
        "        elif achievement_percent >= 85:\n",
        "            return \"good\"\n",
        "        elif achievement_percent >= 70:\n",
        "            return \"fair\"\n",
        "        else:\n",
        "            return \"needs_improvement\"\n",
        "\n",
        "    def _analyze_trends(self, current_data, historical_data: List) -> Dict:\n",
        "        \"\"\"Analyze trends over time\"\"\"\n",
        "\n",
        "        if len(historical_data) < 2:\n",
        "            return {\"message\": \"Insufficient data for trend analysis\"}\n",
        "\n",
        "        # Include current week in analysis\n",
        "        all_data = [current_data] + historical_data\n",
        "        all_data.sort(key=lambda x: x.week_start)  # Sort chronologically\n",
        "\n",
        "        trends = {}\n",
        "\n",
        "        # Steps trend\n",
        "        steps_data = [w.avg_daily_steps for w in all_data]\n",
        "        trends['steps'] = self._calculate_trend(steps_data, \"steps/day\")\n",
        "\n",
        "        # Zone minutes trend\n",
        "        zone_data = [w.zone_minutes for w in all_data]\n",
        "        trends['zone_minutes'] = self._calculate_trend(zone_data, \"minutes/week\")\n",
        "\n",
        "        # Sleep trend\n",
        "        sleep_data = [w.avg_sleep for w in all_data]\n",
        "        trends['sleep'] = self._calculate_trend(sleep_data, \"hours/night\")\n",
        "\n",
        "        # Mood trend\n",
        "        mood_data = [w.avg_mood for w in all_data if w.avg_mood > 0]\n",
        "        if mood_data:\n",
        "            trends['mood'] = self._calculate_trend(mood_data, \"mood score\")\n",
        "\n",
        "        # Food trends\n",
        "        food_categories = ['dairy', 'legumes', 'meat', 'fruits', 'vegetables', 'grains', 'nuts_seeds', 'water_glasses']\n",
        "        for category in food_categories:\n",
        "            weekly_key = f'weekly_{category}'\n",
        "            category_data = []\n",
        "            for w in all_data:\n",
        "                if hasattr(w, 'weekly_food_totals') and weekly_key in w.weekly_food_totals:\n",
        "                    category_data.append(w.weekly_food_totals[weekly_key])\n",
        "\n",
        "            if len(category_data) >= 2:\n",
        "                trends[category] = self._calculate_trend(category_data, f\"servings/week\")\n",
        "\n",
        "        return trends\n",
        "\n",
        "    def _calculate_trend(self, data: List[float], unit: str) -> Dict:\n",
        "        \"\"\"Calculate trend direction and magnitude\"\"\"\n",
        "        if len(data) < 2:\n",
        "            return {\"direction\": \"insufficient_data\"}\n",
        "\n",
        "        # Calculate linear trend\n",
        "        x = np.arange(len(data))\n",
        "        slope, intercept = np.polyfit(x, data, 1)\n",
        "\n",
        "        # Determine trend direction\n",
        "        if slope > 0.1:\n",
        "            direction = \"improving\"\n",
        "        elif slope < -0.1:\n",
        "            direction = \"declining\"\n",
        "        else:\n",
        "            direction = \"stable\"\n",
        "\n",
        "        # Calculate percentage change from first to last\n",
        "        if data[0] != 0:\n",
        "            percent_change = ((data[-1] - data[0]) / data[0]) * 100\n",
        "        else:\n",
        "            percent_change = 0\n",
        "\n",
        "        return {\n",
        "            'direction': direction,\n",
        "            'slope': slope,\n",
        "            'percent_change': percent_change,\n",
        "            'recent_value': data[-1],\n",
        "            'unit': unit,\n",
        "            'data_points': len(data)\n",
        "        }\n",
        "\n",
        "    def _analyze_progress(self, current_data, historical_data: List) -> Dict:\n",
        "        \"\"\"Analyze progress compared to previous weeks\"\"\"\n",
        "\n",
        "        if not historical_data:\n",
        "            return {\"message\": \"No historical data for comparison\"}\n",
        "\n",
        "        # Compare with last week\n",
        "        last_week = historical_data[0]  # Most recent week\n",
        "\n",
        "        progress = {}\n",
        "\n",
        "        # Steps progress\n",
        "        steps_change = current_data.avg_daily_steps - last_week.avg_daily_steps\n",
        "        progress['steps'] = {\n",
        "            'change': steps_change,\n",
        "            'percent_change': (steps_change / last_week.avg_daily_steps) * 100 if last_week.avg_daily_steps > 0 else 0,\n",
        "            'direction': 'improved' if steps_change > 0 else 'declined' if steps_change < 0 else 'maintained'\n",
        "        }\n",
        "\n",
        "        # Food progress\n",
        "        food_categories = ['dairy', 'legumes', 'meat', 'fruits', 'vegetables', 'grains', 'nuts_seeds', 'water_glasses']\n",
        "        for category in food_categories:\n",
        "            weekly_key = f'weekly_{category}'\n",
        "            if (hasattr(current_data, 'weekly_food_totals') and weekly_key in current_data.weekly_food_totals and\n",
        "                hasattr(last_week, 'weekly_food_totals') and weekly_key in last_week.weekly_food_totals):\n",
        "\n",
        "                current_intake = current_data.weekly_food_totals[weekly_key]\n",
        "                last_intake = last_week.weekly_food_totals[weekly_key]\n",
        "                change = current_intake - last_intake\n",
        "\n",
        "                progress[category] = {\n",
        "                    'change': change,\n",
        "                    'percent_change': (change / last_intake) * 100 if last_intake > 0 else 0,\n",
        "                    'direction': 'improved' if change > 0 else 'declined' if change < 0 else 'maintained'\n",
        "                }\n",
        "\n",
        "        return progress\n",
        "\n",
        "\n",
        "        # Zone minutes progress\n",
        "        zone_change = current_data.zone_minutes - last_week.zone_minutes\n",
        "        progress['zone_minutes'] = {\n",
        "            'change': zone_change,\n",
        "            'percent_change': (zone_change / last_week.zone_minutes) * 100 if last_week.zone_minutes > 0 else 0,\n",
        "            'direction': 'improved' if zone_change > 0 else 'declined' if zone_change < 0 else 'maintained'\n",
        "        }\n",
        "\n",
        "        # Sleep progress\n",
        "        sleep_change = current_data.avg_sleep - last_week.avg_sleep\n",
        "        progress['sleep'] = {\n",
        "            'change': sleep_change,\n",
        "            'percent_change': (sleep_change / last_week.avg_sleep) * 100 if last_week.avg_sleep > 0 else 0,\n",
        "            'direction': 'improved' if sleep_change > 0 else 'declined' if sleep_change < 0 else 'maintained'\n",
        "        }\n",
        "\n",
        "        # Mood progress\n",
        "        if current_data.avg_mood > 0 and last_week.avg_mood > 0:\n",
        "            mood_change = current_data.avg_mood - last_week.avg_mood\n",
        "            progress['mood'] = {\n",
        "                'change': mood_change,\n",
        "                'percent_change': (mood_change / last_week.avg_mood) * 100,\n",
        "                'direction': 'improved' if mood_change > 0 else 'declined' if mood_change < 0 else 'maintained'\n",
        "            }\n",
        "\n",
        "        return progress\n",
        "\n",
        "    def _identify_priority_areas(self, performance: Dict, trends: Dict) -> List[Dict]:\n",
        "        \"\"\"Identify which areas need most attention\"\"\"\n",
        "\n",
        "        priorities = []\n",
        "\n",
        "        # Check each area\n",
        "        for area, perf in performance.items():\n",
        "            if area == 'overall':\n",
        "                continue\n",
        "\n",
        "            priority_score = 0\n",
        "            reasons = []\n",
        "\n",
        "            # Low performance\n",
        "            if perf['achievement_percent'] < 70:\n",
        "                priority_score += 3\n",
        "                reasons.append(f\"Below target ({perf['achievement_percent']:.1f}%)\")\n",
        "            elif perf['achievement_percent'] < 85:\n",
        "                priority_score += 2\n",
        "                reasons.append(f\"Slightly below target ({perf['achievement_percent']:.1f}%)\")\n",
        "\n",
        "            # Declining trend\n",
        "            if area in trends and trends[area].get('direction') == 'declining':\n",
        "                priority_score += 2\n",
        "                reasons.append(\"Declining trend\")\n",
        "\n",
        "            # Add to priorities if score is significant\n",
        "            if priority_score > 0:\n",
        "                priorities.append({\n",
        "                    'area': area,\n",
        "                    'priority_score': priority_score,\n",
        "                    'reasons': reasons,\n",
        "                    'current_performance': perf['achievement_percent']\n",
        "                })\n",
        "\n",
        "        # Sort by priority score (highest first)\n",
        "        priorities.sort(key=lambda x: x['priority_score'], reverse=True)\n",
        "\n",
        "        return priorities\n",
        "\n",
        "\n",
        "    def _get_performance_status(self, achievement_percent: float) -> str:\n",
        "        \"\"\"Get performance status based on achievement percentage\"\"\"\n",
        "        if achievement_percent >= 100:\n",
        "            return \"excellent\"\n",
        "        elif achievement_percent >= 85:\n",
        "            return \"good\"\n",
        "        elif achievement_percent >= 70:\n",
        "            return \"fair\"\n",
        "        else:\n",
        "            return \"needs_improvement\"\n",
        "\n",
        "    def _get_calorie_status(self, achievement_percent: float) -> str:\n",
        "        \"\"\"Get calorie status (different logic as too many calories is bad)\"\"\"\n",
        "        if 90 <= achievement_percent <= 110:\n",
        "            return \"excellent\"\n",
        "        elif 80 <= achievement_percent <= 120:\n",
        "            return \"good\"\n",
        "        elif 70 <= achievement_percent <= 130:\n",
        "            return \"fair\"\n",
        "        else:\n",
        "            return \"needs_adjustment\"\n",
        "\n",
        "    def generate_insights(self, analysis: Dict) -> List[str]:\n",
        "        \"\"\"Generate human-readable insights from analysis\"\"\"\n",
        "\n",
        "        insights = []\n",
        "\n",
        "        # Overall performance insight\n",
        "        overall_score = analysis['performance']['overall']['score']\n",
        "        if overall_score >= 90:\n",
        "            insights.append(\"🎉 Excellent overall performance this week!\")\n",
        "        elif overall_score >= 75:\n",
        "            insights.append(\"👍 Good overall performance with room for improvement\")\n",
        "        else:\n",
        "            insights.append(\"⚠️ Several areas need attention this week\")\n",
        "\n",
        "        # Priority area insights\n",
        "        priorities = analysis['priorities']\n",
        "        if priorities:\n",
        "            top_priority = priorities[0]\n",
        "            insights.append(f\"🎯 Top priority: {top_priority['area'].replace('_', ' ').title()}\")\n",
        "\n",
        "        # Trend insights - FIXED\n",
        "        trends = analysis.get('trends', {})\n",
        "        improving_areas = []\n",
        "        declining_areas = []\n",
        "\n",
        "        # Check if trends is a dictionary before iterating\n",
        "        if isinstance(trends, dict):\n",
        "            for area, trend in trends.items():\n",
        "                # Check if trend is a dictionary (not a string)\n",
        "                if isinstance(trend, dict):\n",
        "                    if trend.get('direction') == 'improving':\n",
        "                        improving_areas.append(area)\n",
        "                    elif trend.get('direction') == 'declining':\n",
        "                        declining_areas.append(area)\n",
        "                elif isinstance(trend, str):\n",
        "                    # Handle case where trend is just a string\n",
        "                    if trend == 'improving':\n",
        "                        improving_areas.append(area)\n",
        "                    elif trend == 'declining':\n",
        "                        declining_areas.append(area)\n",
        "\n",
        "        if improving_areas:\n",
        "            insights.append(f\"📈 Improving: {', '.join(improving_areas)}\")\n",
        "\n",
        "        if declining_areas:\n",
        "            insights.append(f\"📉 Needs attention: {', '.join(declining_areas)}\")\n",
        "\n",
        "        # Progress insights - FIXED\n",
        "        progress = analysis.get('progress', {})\n",
        "        if isinstance(progress, dict):\n",
        "            for area, prog in progress.items():\n",
        "                if isinstance(prog, dict):\n",
        "                    if prog.get('direction') == 'improved' and prog.get('percent_change', 0) > 10:\n",
        "                        insights.append(f\"🚀 Great improvement in {area}: +{prog['percent_change']:.1f}%\")\n",
        "\n",
        "        return insights\n",
        "\n",
        "# ========================================================================\n",
        "# CORRECTED USAGE EXAMPLE FOR COLAB\n",
        "# ========================================================================\n",
        "\n",
        "def example_analytics():\n",
        "    \"\"\"Example of using the analytics engine - CORRECTED FOR COLAB\"\"\"\n",
        "\n",
        "    # Use the data manager and classes from the previous cell (Step 1)\n",
        "    # No import needed since they're already defined in the notebook\n",
        "\n",
        "    data_manager = HealthDataManager()\n",
        "    analytics = HealthAnalytics(data_manager)\n",
        "\n",
        "    # Example current week data\n",
        "    current_week_data = WeeklyHealthData(\"user_123\", \"2024-01-22\", {\n",
        "        'total_steps': 72000,  # Improved from last week\n",
        "        'zone_minutes': 160,   # Good improvement\n",
        "        'sleep_hours': [7.8, 7.2, 8.1, 7.5, 7.0, 8.3, 7.9],\n",
        "        'exercise_sessions': [\n",
        "            {'type': 'running', 'duration': 35},\n",
        "            {'type': 'yoga', 'duration': 45},\n",
        "            {'type': 'cycling', 'duration': 40}\n",
        "        ],\n",
        "        'mood_scores': [8, 7, 8, 7, 6, 9, 8],\n",
        "        'food_data': []  # Simplified for example\n",
        "    })\n",
        "\n",
        "    # Perform analysis\n",
        "    analysis = analytics.analyze_weekly_performance(\"user_123\", current_week_data)\n",
        "\n",
        "    # Generate insights\n",
        "    insights = analytics.generate_insights(analysis)\n",
        "\n",
        "    print(\"=== WEEKLY HEALTH ANALYSIS ===\")\n",
        "    print(f\"User: {analysis['user_id']}\")\n",
        "    print(f\"Week: {analysis['week_start']}\")\n",
        "    print(f\"Overall Score: {analysis['performance']['overall']['score']:.1f}/100\")\n",
        "\n",
        "    print(\"\\n=== KEY INSIGHTS ===\")\n",
        "    for insight in insights:\n",
        "        print(insight)\n",
        "\n",
        "    print(\"\\n=== PERFORMANCE BREAKDOWN ===\")\n",
        "    for area, perf in analysis['performance'].items():\n",
        "        if area != 'overall':\n",
        "            print(f\"{area.replace('_', ' ').title()}: {perf['achievement_percent']:.1f}% ({perf['status']})\")\n",
        "\n",
        "    if analysis['priorities']:\n",
        "        print(\"\\n=== PRIORITY AREAS ===\")\n",
        "        for priority in analysis['priorities'][:3]:  # Top 3\n",
        "            print(f\"{priority['area'].replace('_', ' ').title()}: {', '.join(priority['reasons'])}\")\n",
        "\n",
        "    return analysis\n",
        "\n",
        "# Test the analytics (optional - run this to test)\n",
        "if __name__ == \"__main__\":\n",
        "    analysis = example_analytics()"
      ],
      "metadata": {
        "id": "gsqEhy7OyYq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================================\n",
        "# STEP 3: LLM Integration with Your Health Model\n",
        "# ========================================================================\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from langchain.vectorstores import FAISS\n",
        "# Updated import to fix deprecation warning\n",
        "try:\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "except ImportError:\n",
        "    from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import re\n",
        "from typing import Dict, List\n",
        "import json\n",
        "\n",
        "class HealthLLMProcessor:\n",
        "    \"\"\"Enhanced LLM processor for health analytics with memory and learning\"\"\"\n",
        "\n",
        "    def __init__(self, base_model_id: str, adapter_path: str, vector_db_path: str, data_manager):\n",
        "        self.data_manager = data_manager\n",
        "        self._load_model(base_model_id, adapter_path)\n",
        "        self._load_vector_db(vector_db_path)\n",
        "\n",
        "    def _load_model(self, base_model_id: str, adapter_path: str):\n",
        "        \"\"\"Load your fine-tuned model\"\"\"\n",
        "        print(\"Loading health LLM...\")\n",
        "\n",
        "        # Your exact configuration\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "        )\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Load model with quantization\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_id,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True,\n",
        "            quantization_config=bnb_config,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Load the LoRA adapter\n",
        "        self.model = PeftModel.from_pretrained(self.model, adapter_path)\n",
        "        self.model.eval()\n",
        "        print(\"✅ Model loaded successfully\")\n",
        "\n",
        "    def _load_vector_db(self, vector_db_path: str):\n",
        "        \"\"\"Load your vector database\"\"\"\n",
        "        print(\"Loading vector database...\")\n",
        "        self.embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "        self.vectorstore = FAISS.load_local(\n",
        "            vector_db_path,\n",
        "            embeddings=self.embedding_model,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "        print(\"✅ Vector database loaded successfully\")\n",
        "\n",
        "    def generate_weekly_recommendations(self, user_id: str, analysis_data: Dict) -> Dict:\n",
        "        \"\"\"Generate comprehensive weekly recommendations based on analysis\"\"\"\n",
        "\n",
        "        # Get user context\n",
        "        user_context = self._build_user_context(user_id)\n",
        "\n",
        "        # Get relevant knowledge from vector DB\n",
        "        knowledge_context = self._get_relevant_knowledge(analysis_data)\n",
        "\n",
        "        # Build prompt with all context\n",
        "        prompt = self._build_comprehensive_prompt(user_id, analysis_data, user_context, knowledge_context)\n",
        "\n",
        "        # Generate recommendations\n",
        "        raw_response = self._generate_response(prompt)\n",
        "\n",
        "        # Parse and structure the response\n",
        "        structured_recommendations = self._parse_recommendations(raw_response)\n",
        "\n",
        "        # Learn from this interaction\n",
        "        self._update_user_learning(user_id, analysis_data, structured_recommendations)\n",
        "\n",
        "        return structured_recommendations\n",
        "\n",
        "    def _build_user_context(self, user_id: str) -> str:\n",
        "        \"\"\"Build context about user's history and preferences\"\"\"\n",
        "\n",
        "        # Get user profile\n",
        "        profile = self.data_manager.load_user_profile(user_id)\n",
        "\n",
        "        # Get recent recommendations and feedback\n",
        "        recent_recs = self.data_manager.load_user_recommendations(user_id, num_weeks=2)\n",
        "\n",
        "        # Get historical data for patterns\n",
        "        historical_data = self.data_manager.load_user_weekly_data(user_id, num_weeks=4)\n",
        "\n",
        "        context = f\"USER PROFILE:\\n\"\n",
        "\n",
        "        if profile:\n",
        "            context += f\"- Targets: {profile.targets}\\n\"\n",
        "            context += f\"- Preferences: {profile.preferences}\\n\"\n",
        "            context += f\"- Response Patterns: {profile.response_patterns}\\n\"\n",
        "\n",
        "        if recent_recs:\n",
        "            context += f\"\\nRECENT RECOMMENDATIONS:\\n\"\n",
        "            for rec in recent_recs[:2]:  # Last 2 weeks\n",
        "                context += f\"Week {rec.week_start}:\\n\"\n",
        "                for area, recommendation in rec.recommendations.items():\n",
        "                    context += f\"  {area}: {recommendation}\\n\"\n",
        "                if rec.user_feedback.get('followed_exercise'):\n",
        "                    context += f\"  User followed exercise advice: {rec.user_feedback['followed_exercise']}\\n\"\n",
        "\n",
        "        if historical_data:\n",
        "            context += f\"\\nHISTORICAL PATTERNS:\\n\"\n",
        "            # Calculate what user typically achieves\n",
        "            avg_steps = sum([w.avg_daily_steps for w in historical_data]) / len(historical_data)\n",
        "            avg_zone = sum([w.zone_minutes for w in historical_data]) / len(historical_data)\n",
        "            context += f\"- Typical daily steps: {avg_steps:.0f}\\n\"\n",
        "            context += f\"- Typical zone minutes: {avg_zone:.0f}\\n\"\n",
        "\n",
        "        return context\n",
        "\n",
        "    def _get_relevant_knowledge(self, analysis_data: Dict) -> str:\n",
        "        \"\"\"Get relevant knowledge from vector database based on priority areas\"\"\"\n",
        "\n",
        "        priorities = analysis_data.get('priorities', [])\n",
        "\n",
        "        if not priorities:\n",
        "            # General health query\n",
        "            search_query = \"healthy lifestyle exercise nutrition sleep wellness\"\n",
        "        else:\n",
        "            # Build query based on priority areas\n",
        "            priority_areas = [p['area'] for p in priorities[:2]]  # Top 2 priorities\n",
        "            search_query = \" \".join(priority_areas) + \" improvement recommendations health\"\n",
        "\n",
        "        # Search vector database\n",
        "        relevant_docs = self.vectorstore.similarity_search(search_query, k=4)\n",
        "\n",
        "        # Combine relevant knowledge\n",
        "        knowledge = \"\\n---\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "        return knowledge[:2000]  # Limit context length\n",
        "\n",
        "    def _build_comprehensive_prompt(self, user_id: str, analysis_data: Dict, user_context: str, knowledge_context: str) -> str:\n",
        "        \"\"\"Build comprehensive prompt with all context\"\"\"\n",
        "\n",
        "        performance = analysis_data.get('performance', {})\n",
        "        priorities = analysis_data.get('priorities', [])\n",
        "        trends = analysis_data.get('trends', {})\n",
        "        progress = analysis_data.get('progress', {})\n",
        "\n",
        "        prompt = f\"\"\"<|system|>\n",
        "You are an expert health and wellness AI assistant. You provide personalized recommendations based on comprehensive health data analysis.\n",
        "\n",
        "IMPORTANT INSTRUCTIONS:\n",
        "1. Provide recommendations in EXACTLY 4 sections: Food Recommendation, Physical Exercise, Sleep and Recovery, Overall Suggestion\n",
        "2. Use bullet points with a), b), c), d) format\n",
        "3. Base recommendations on the user's specific data, priorities, and historical patterns\n",
        "4. Consider what has worked/not worked for this user before\n",
        "5. Make recommendations progressive and achievable\n",
        "6. End with \"### End ###\"\n",
        "\n",
        "{user_context}\n",
        "\n",
        "CURRENT WEEK ANALYSIS:\n",
        "Overall Performance Score: {performance.get('overall', {}).get('score', 0):.1f}/100\n",
        "\n",
        "PERFORMANCE BREAKDOWN:\n",
        "- Steps: {performance.get('steps', {}).get('achievement_percent', 0):.1f}% of target\n",
        "- Zone Minutes: {performance.get('zone_minutes', {}).get('achievement_percent', 0):.1f}% of target\n",
        "- Sleep: {performance.get('sleep', {}).get('achievement_percent', 0):.1f}% of target\n",
        "- Exercise Sessions: {performance.get('exercise_sessions', {}).get('achievement_percent', 0):.1f}% of target\n",
        "\n",
        "PRIORITY AREAS NEEDING ATTENTION:\n",
        "{self._format_priorities(priorities)}\n",
        "\n",
        "TRENDS:\n",
        "{self._format_trends(trends)}\n",
        "\n",
        "PROGRESS FROM LAST WEEK:\n",
        "{self._format_progress(progress)}\n",
        "\n",
        "KNOWLEDGE BASE CONTEXT:\n",
        "{knowledge_context}\n",
        "<|/system|>\n",
        "\n",
        "<|user|>\n",
        "Based on my comprehensive health analysis above, provide personalized recommendations for the upcoming week. Focus on my priority areas while building on what has worked for me before.\n",
        "<|/user|>\n",
        "\n",
        "<|assistant|>\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def _format_priorities(self, priorities: List[Dict]) -> str:\n",
        "        \"\"\"Format priority areas for prompt\"\"\"\n",
        "        if not priorities:\n",
        "            return \"No major areas of concern - maintain current performance\"\n",
        "\n",
        "        formatted = \"\"\n",
        "        for i, priority in enumerate(priorities[:3], 1):\n",
        "            formatted += f\"{i}. {priority['area'].replace('_', ' ').title()}: {', '.join(priority['reasons'])}\\n\"\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    def _format_trends(self, trends: Dict) -> str:\n",
        "        \"\"\"Format trends for prompt\"\"\"\n",
        "        if not trends:\n",
        "            return \"Insufficient data for trend analysis\"\n",
        "\n",
        "        formatted = \"\"\n",
        "        for area, trend in trends.items():\n",
        "            # Handle both string and dict trend values\n",
        "            if isinstance(trend, dict):\n",
        "                direction = trend.get('direction', 'stable')\n",
        "                change = trend.get('percent_change', 0)\n",
        "                formatted += f\"- {area.replace('_', ' ').title()}: {direction}\"\n",
        "                if change != 0:\n",
        "                    formatted += f\" ({change:+.1f}%)\"\n",
        "            elif isinstance(trend, str):\n",
        "                # Handle case where trend is just a string\n",
        "                if trend in ['improving', 'declining', 'stable', 'insufficient_data']:\n",
        "                    formatted += f\"- {area.replace('_', ' ').title()}: {trend}\"\n",
        "                else:\n",
        "                    formatted += f\"- {area.replace('_', ' ').title()}: insufficient data\"\n",
        "            else:\n",
        "                # Fallback for any other type\n",
        "                formatted += f\"- {area.replace('_', ' ').title()}: insufficient data\"\n",
        "            formatted += \"\\n\"\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    def _format_progress(self, progress: Dict) -> str:\n",
        "        \"\"\"Format progress for prompt\"\"\"\n",
        "        if not progress:\n",
        "            return \"No previous week for comparison\"\n",
        "\n",
        "        formatted = \"\"\n",
        "        for area, prog in progress.items():\n",
        "            # Handle both string and dict progress values\n",
        "            if isinstance(prog, dict):\n",
        "                direction = prog.get('direction', 'maintained')\n",
        "                change = prog.get('change', 0)\n",
        "                formatted += f\"- {area.replace('_', ' ').title()}: {direction}\"\n",
        "                if change != 0:\n",
        "                    formatted += f\" ({change:+.1f})\"\n",
        "            elif isinstance(prog, str):\n",
        "                # Handle case where progress is just a string\n",
        "                formatted += f\"- {area.replace('_', ' ').title()}: {prog}\"\n",
        "            else:\n",
        "                # Fallback\n",
        "                formatted += f\"- {area.replace('_', ' ').title()}: no data\"\n",
        "            formatted += \"\\n\"\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    def _generate_response(self, prompt: str) -> str:\n",
        "        \"\"\"Generate response using your model\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=500,\n",
        "                min_new_tokens=100,\n",
        "                temperature=0.3,\n",
        "                top_p=0.85,\n",
        "                top_k=40,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=[\n",
        "                    self.tokenizer.encode(\"### End ###\")[0] if \"### End ###\" in self.tokenizer.get_vocab() else self.tokenizer.eos_token_id,\n",
        "                    self.tokenizer.eos_token_id\n",
        "                ],\n",
        "                early_stopping=True,\n",
        "                no_repeat_ngram_size=3\n",
        "            )\n",
        "\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract only the assistant's response\n",
        "        if \"<|assistant|>\" in response:\n",
        "            generated_text = response.split(\"<|assistant|>\")[-1].strip()\n",
        "        else:\n",
        "            generated_text = response[len(prompt):].strip()\n",
        "\n",
        "        # Clean up the response\n",
        "        generated_text = re.sub(r'<\\|.*?\\|>', '', generated_text)\n",
        "        generated_text = generated_text.strip()\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "    def _parse_recommendations(self, raw_response: str) -> Dict:\n",
        "        \"\"\"Parse the LLM response into structured recommendations\"\"\"\n",
        "\n",
        "        sections = {\n",
        "            'food_recommendation': '',\n",
        "            'physical_exercise': '',\n",
        "            'sleep_and_recovery': '',\n",
        "            'overall_suggestion': '',\n",
        "            'raw_response': raw_response\n",
        "        }\n",
        "\n",
        "        # Extract each section\n",
        "        patterns = {\n",
        "            'food_recommendation': r'1\\.\\s*Food Recommendation[:\\s]*(.*?)(?=2\\.\\s*Physical Exercise|$)',\n",
        "            'physical_exercise': r'2\\.\\s*Physical Exercise[:\\s]*(.*?)(?=3\\.\\s*Sleep and Recovery|$)',\n",
        "            'sleep_and_recovery': r'3\\.\\s*Sleep and Recovery[:\\s]*(.*?)(?=4\\.\\s*Overall Suggestion|$)',\n",
        "            'overall_suggestion': r'4\\.\\s*Overall Suggestion[:\\s]*(.*?)(?=### End ###|$)'\n",
        "        }\n",
        "\n",
        "        for section, pattern in patterns.items():\n",
        "            match = re.search(pattern, raw_response, re.DOTALL | re.IGNORECASE)\n",
        "            if match:\n",
        "                sections[section] = match.group(1).strip()\n",
        "\n",
        "        return sections\n",
        "\n",
        "    def _update_user_learning(self, user_id: str, analysis_data: Dict, recommendations: Dict):\n",
        "        \"\"\"Update user learning patterns based on new recommendations\"\"\"\n",
        "\n",
        "        # This will help improve future recommendations\n",
        "        profile = self.data_manager.load_user_profile(user_id)\n",
        "\n",
        "        if profile:\n",
        "            # Track recommendation types given\n",
        "            priorities = analysis_data.get('priorities', [])\n",
        "            if priorities:\n",
        "                top_priority = priorities[0]['area']\n",
        "                if top_priority not in profile.response_patterns.get('areas_worked_on', []):\n",
        "                    if 'areas_worked_on' not in profile.response_patterns:\n",
        "                        profile.response_patterns['areas_worked_on'] = []\n",
        "                    profile.response_patterns['areas_worked_on'].append(top_priority)\n",
        "\n",
        "            # Save updated profile\n",
        "            self.data_manager.save_user_profile(profile)\n",
        "\n",
        "    def update_recommendation_feedback(self, user_id: str, week_start: str, feedback: Dict):\n",
        "        \"\"\"Update feedback on recommendations to improve future suggestions\"\"\"\n",
        "\n",
        "        # Load the recommendation\n",
        "        recommendations = self.data_manager.load_user_recommendations(user_id, num_weeks=4)\n",
        "\n",
        "        for rec in recommendations:\n",
        "            if rec.week_start == week_start:\n",
        "                rec.update_feedback(feedback)\n",
        "                self.data_manager.save_recommendations(rec)\n",
        "\n",
        "                # Update user profile learning patterns\n",
        "                profile = self.data_manager.load_user_profile(user_id)\n",
        "                if profile:\n",
        "                    # Update response patterns based on feedback\n",
        "                    if feedback.get('followed_exercise'):\n",
        "                        profile.response_patterns['follows_exercise_recs'] = min(1.0,\n",
        "                            profile.response_patterns.get('follows_exercise_recs', 0.5) + 0.1)\n",
        "\n",
        "                    if feedback.get('followed_nutrition'):\n",
        "                        profile.response_patterns['follows_nutrition_recs'] = min(1.0,\n",
        "                            profile.response_patterns.get('follows_nutrition_recs', 0.5) + 0.1)\n",
        "\n",
        "                    if feedback.get('followed_sleep'):\n",
        "                        profile.response_patterns['follows_sleep_recs'] = min(1.0,\n",
        "                            profile.response_patterns.get('follows_sleep_recs', 0.5) + 0.1)\n",
        "\n",
        "                    self.data_manager.save_user_profile(profile)\n",
        "                break\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 3 USAGE EXAMPLE (COLAB FIXED)\n",
        "# ========================================================================\n",
        "\n",
        "def example_llm_processing():\n",
        "    \"\"\"Example of using the enhanced LLM processor - CORRECTED FOR COLAB\"\"\"\n",
        "\n",
        "    # No imports needed - classes are already defined in previous cells\n",
        "\n",
        "    # Initialize components\n",
        "    data_manager = HealthDataManager()\n",
        "    analytics = HealthAnalytics(data_manager)\n",
        "\n",
        "    # Create user profile first\n",
        "    user_id = \"user_123\"\n",
        "    profile = UserProfile(user_id)\n",
        "    profile.update_targets({\n",
        "        'daily_steps': 10000,\n",
        "        'weekly_zone_minutes': 150,\n",
        "        'daily_sleep_hours': 8,\n",
        "        'weekly_exercise_sessions': 3,\n",
        "        'daily_calories': 2000\n",
        "    })\n",
        "    data_manager.save_user_profile(profile)\n",
        "    print(f\"✅ Created user profile for {user_id}\")\n",
        "\n",
        "    # Create some historical data for better analysis\n",
        "    historical_week = WeeklyHealthData(user_id, \"2024-01-22\", {\n",
        "        'total_steps': 62000,\n",
        "        'zone_minutes': 100,\n",
        "        'sleep_hours': [6.8, 6.0, 7.5, 6.5, 6.2, 8.0, 7.2],\n",
        "        'exercise_sessions': [{'type': 'walking', 'duration': 30}],\n",
        "        'mood_scores': [6, 5, 7, 6, 4, 7, 6],\n",
        "        'food_data': []\n",
        "    })\n",
        "    data_manager.save_weekly_data(historical_week)\n",
        "    print(\"✅ Created historical data\")\n",
        "\n",
        "    try:\n",
        "        # Initialize LLM processor with your model paths\n",
        "        llm_processor = HealthLLMProcessor(\n",
        "            base_model_id=\"ContactDoctor/Bio-Medical-Llama-3-8B\",\n",
        "            adapter_path=\"AnjaliNV/WellBeing_LLM\",\n",
        "            vector_db_path=\"/content/drive/MyDrive/rag_index\",  # Your vector DB path\n",
        "            data_manager=data_manager\n",
        "        )\n",
        "        print(\"✅ LLM processor initialized\")\n",
        "\n",
        "        # Current week data\n",
        "        current_week_data = WeeklyHealthData(user_id, \"2024-01-29\", {\n",
        "            'total_steps': 58000,  # Below target\n",
        "            'zone_minutes': 90,    # Below target\n",
        "            'sleep_hours': [6.5, 6.2, 7.8, 6.8, 6.0, 8.2, 7.5],\n",
        "            'exercise_sessions': [\n",
        "                {'type': 'yoga', 'duration': 45}\n",
        "            ],\n",
        "            'mood_scores': [6, 5, 7, 6, 5, 8, 7],\n",
        "            'food_data': []\n",
        "        })\n",
        "        data_manager.save_weekly_data(current_week_data)\n",
        "        print(\"✅ Created current week data\")\n",
        "\n",
        "        # Analyze the week\n",
        "        analysis = analytics.analyze_weekly_performance(user_id, current_week_data)\n",
        "        print(\"✅ Analysis completed\")\n",
        "\n",
        "        # Check for analysis errors\n",
        "        if 'error' in analysis:\n",
        "            print(f\"❌ Analysis error: {analysis['error']}\")\n",
        "            return None\n",
        "\n",
        "        # Generate recommendations\n",
        "        print(\"🤖 Generating recommendations with LLM...\")\n",
        "        recommendations = llm_processor.generate_weekly_recommendations(user_id, analysis)\n",
        "        print(\"✅ Recommendations generated\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"=== COMPREHENSIVE WEEKLY RECOMMENDATIONS ===\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"👤 User: {user_id}\")\n",
        "        print(f\"📅 Week: {current_week_data.week_start}\")\n",
        "        print(f\"🎯 Overall Score: {analysis['performance']['overall']['score']:.1f}/100\")\n",
        "\n",
        "        print(\"\\n📊 PRIORITY AREAS:\")\n",
        "        if analysis['priorities']:\n",
        "            for priority in analysis['priorities'][:2]:\n",
        "                print(f\"• {priority['area'].replace('_', ' ').title()}: {', '.join(priority['reasons'])}\")\n",
        "        else:\n",
        "            print(\"• No major priority areas - maintain current performance\")\n",
        "\n",
        "        print(\"\\n🍎 FOOD RECOMMENDATIONS:\")\n",
        "        print(recommendations.get('food_recommendation', 'No specific food recommendations available'))\n",
        "\n",
        "        print(\"\\n💪 EXERCISE RECOMMENDATIONS:\")\n",
        "        print(recommendations.get('physical_exercise', 'No specific exercise recommendations available'))\n",
        "\n",
        "        print(\"\\n😴 SLEEP & RECOVERY:\")\n",
        "        print(recommendations.get('sleep_and_recovery', 'No specific sleep recommendations available'))\n",
        "\n",
        "        print(\"\\n🎯 OVERALL SUGGESTIONS:\")\n",
        "        print(recommendations.get('overall_suggestion', 'No specific overall suggestions available'))\n",
        "\n",
        "        # Save the recommendations\n",
        "        weekly_recs = WeeklyRecommendations(user_id, current_week_data.week_start, {\n",
        "            'food': recommendations.get('food_recommendation', ''),\n",
        "            'exercise': recommendations.get('physical_exercise', ''),\n",
        "            'sleep': recommendations.get('sleep_and_recovery', ''),\n",
        "            'overall': recommendations.get('overall_suggestion', '')\n",
        "        })\n",
        "\n",
        "        data_manager.save_recommendations(weekly_recs)\n",
        "        print(\"\\n✅ Recommendations saved for future learning\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during LLM processing: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Return a fallback recommendation structure\n",
        "        return {\n",
        "            'food_recommendation': 'Error generating recommendations - please try again',\n",
        "            'physical_exercise': 'Error generating recommendations - please try again',\n",
        "            'sleep_and_recovery': 'Error generating recommendations - please try again',\n",
        "            'overall_suggestion': 'Error generating recommendations - please try again',\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "# Test the LLM processing (optional - uncomment to test)\n",
        "# if __name__ == \"__main__\":\n",
        "#     recommendations = example_llm_processing()"
      ],
      "metadata": {
        "id": "XYVeKQzS0TEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================================\n",
        "# SYSTEM THAT ACTUALLY USES YOUR LLM FOR RECOMMENDATIONS\n",
        "# ========================================================================\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from datetime import datetime\n",
        "\n",
        "class WorkingLLMHealthSystem:\n",
        "    \"\"\"Health system that actually loads and uses your LLM\"\"\"\n",
        "\n",
        "    def __init__(self, base_model_id: str, adapter_path: str, vector_db_path: str, data_dir: str = \"health_data\"):\n",
        "        \"\"\"Initialize system with working LLM\"\"\"\n",
        "\n",
        "        print(\"🚀 Initializing Health System with LLM...\")\n",
        "\n",
        "        # Initialize basic components first\n",
        "        self.data_manager = HealthDataManager(data_dir)\n",
        "        self.analytics = HealthAnalytics(self.data_manager)\n",
        "        self.UserProfile = UserProfile\n",
        "        self.WeeklyHealthData = WeeklyHealthData\n",
        "        self.WeeklyRecommendations = WeeklyRecommendations\n",
        "\n",
        "        # Clear GPU memory first\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            print(f\"🧹 GPU memory cleared. Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
        "\n",
        "        # Load LLM components\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.vectorstore = None\n",
        "\n",
        "        self._load_llm_components(base_model_id, adapter_path, vector_db_path)\n",
        "\n",
        "    def _load_llm_components(self, base_model_id: str, adapter_path: str, vector_db_path: str):\n",
        "        \"\"\"Load LLM components step by step\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Step 1: Load vector database (lightweight)\n",
        "            print(\"📚 Loading vector database...\")\n",
        "            try:\n",
        "                from langchain_huggingface import HuggingFaceEmbeddings\n",
        "            except ImportError:\n",
        "                from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "            from langchain.vectorstores import FAISS\n",
        "\n",
        "            embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "            self.vectorstore = FAISS.load_local(\n",
        "                vector_db_path,\n",
        "                embeddings=embedding_model,\n",
        "                allow_dangerous_deserialization=True\n",
        "            )\n",
        "            print(\"✅ Vector database loaded successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Vector database failed: {str(e)}\")\n",
        "            self.vectorstore = None\n",
        "\n",
        "        try:\n",
        "            # Step 2: Load tokenizer (lightweight)\n",
        "            print(\"🔤 Loading tokenizer...\")\n",
        "            from transformers import AutoTokenizer\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            print(\"✅ Tokenizer loaded successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Tokenizer loading failed: {str(e)}\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Step 3: Load model (heavy - with optimizations)\n",
        "            print(\"🤖 Loading your fine-tuned model...\")\n",
        "            from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "            from peft import PeftModel\n",
        "\n",
        "            # Aggressive quantization config\n",
        "            bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_compute_dtype=torch.float16,\n",
        "                llm_int8_enable_fp32_cpu_offload=True\n",
        "            )\n",
        "\n",
        "            # Try to load with CPU offloading\n",
        "            try:\n",
        "                print(\"  🔄 Attempting GPU + CPU hybrid loading...\")\n",
        "                self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                    base_model_id,\n",
        "                    device_map=\"auto\",\n",
        "                    torch_dtype=torch.float16,\n",
        "                    low_cpu_mem_usage=True,\n",
        "                    quantization_config=bnb_config,\n",
        "                    trust_remote_code=True,\n",
        "                    max_memory={0: \"6GB\", \"cpu\": \"8GB\"}  # Limit GPU usage\n",
        "                )\n",
        "\n",
        "                # Load LoRA adapter\n",
        "                self.model = PeftModel.from_pretrained(self.model, adapter_path)\n",
        "                self.model.eval()\n",
        "\n",
        "                print(\"✅ Model loaded successfully with GPU + CPU hybrid!\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠️ Hybrid loading failed: {str(e)}\")\n",
        "                print(\"  🔄 Trying CPU-only loading...\")\n",
        "\n",
        "                # Fallback to CPU-only\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                    base_model_id,\n",
        "                    device_map=\"cpu\",\n",
        "                    torch_dtype=torch.float16,\n",
        "                    low_cpu_mem_usage=True,\n",
        "                    trust_remote_code=True\n",
        "                )\n",
        "\n",
        "                self.model = PeftModel.from_pretrained(self.model, adapter_path)\n",
        "                self.model.eval()\n",
        "\n",
        "                print(\"✅ Model loaded successfully on CPU!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Model loading completely failed: {str(e)}\")\n",
        "            self.model = None\n",
        "\n",
        "    def process_weekly_data(self, user_id: str, week_data: Dict, targets: Dict = None) -> Dict:\n",
        "        \"\"\"Process data and generate LLM recommendations\"\"\"\n",
        "\n",
        "        try:\n",
        "            print(f\"📊 Processing data for user: {user_id}\")\n",
        "\n",
        "            # Basic data processing (same as before)\n",
        "            profile = self.data_manager.load_user_profile(user_id)\n",
        "            if not profile:\n",
        "                profile = self.UserProfile(user_id)\n",
        "                if targets:\n",
        "                    profile.update_targets(targets)\n",
        "                self.data_manager.save_user_profile(profile)\n",
        "\n",
        "            # Create simplified weekly data for analytics\n",
        "            week_start = week_data.get('week_start', datetime.now().strftime('%Y-%m-%d'))\n",
        "            simplified_data = {\n",
        "                'week_start': week_start,\n",
        "                'total_steps': week_data.get('total_steps', 0),\n",
        "                'zone_minutes': week_data.get('zone_minutes', 0),\n",
        "                'sleep_hours': week_data.get('sleep_hours', []),\n",
        "                'mood_scores': week_data.get('mood_scores', []),\n",
        "                'exercise_sessions': week_data.get('exercise_sessions', []),\n",
        "                'food_data': []\n",
        "            }\n",
        "\n",
        "            weekly_health_data = self.WeeklyHealthData(user_id, week_start, simplified_data)\n",
        "            self.data_manager.save_weekly_data(weekly_health_data)\n",
        "\n",
        "            # Perform analysis\n",
        "            analysis = self.analytics.analyze_weekly_performance(user_id, weekly_health_data)\n",
        "            print(\"✅ Analysis completed\")\n",
        "\n",
        "            # Generate recommendations using LLM\n",
        "            if self.model and self.tokenizer:\n",
        "                print(\"🤖 Generating LLM recommendations...\")\n",
        "                recommendations = self._generate_llm_recommendations(analysis, week_data, profile)\n",
        "                print(\"✅ LLM recommendations generated!\")\n",
        "            else:\n",
        "                print(\"⚠️ LLM not available, using smart fallback...\")\n",
        "                recommendations = self._generate_smart_fallback(analysis, week_data)\n",
        "\n",
        "            # Save and return results\n",
        "            weekly_recs = self.WeeklyRecommendations(user_id, week_start, {\n",
        "                'food': recommendations.get('food_recommendation', ''),\n",
        "                'exercise': recommendations.get('physical_exercise', ''),\n",
        "                'sleep': recommendations.get('sleep_and_recovery', ''),\n",
        "                'overall': recommendations.get('overall_suggestion', ''),\n",
        "            })\n",
        "\n",
        "            self.data_manager.save_recommendations(weekly_recs)\n",
        "\n",
        "            return {\n",
        "                'user_id': user_id,\n",
        "                'week_start': week_start,\n",
        "                'analysis': analysis,\n",
        "                'recommendations': recommendations,\n",
        "                'insights': self.analytics.generate_insights(analysis),\n",
        "                'week_summary': self._create_week_summary(weekly_health_data, week_data),\n",
        "                'llm_used': self.model is not None  # Tell user if LLM was used\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error in process_weekly_data: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return self._create_error_response(user_id, week_data, str(e))\n",
        "\n",
        "    def _generate_llm_recommendations(self, analysis: Dict, week_data: Dict, profile) -> Dict:\n",
        "        \"\"\"Generate recommendations using your actual LLM\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Build context for LLM\n",
        "            context = self._build_llm_context(analysis, week_data, profile)\n",
        "\n",
        "            # Create prompt for your model\n",
        "            prompt = f\"\"\"<|system|>\n",
        "You are a expert health and wellness advisor. Provide personalized recommendations based on the user's health data analysis.\n",
        "\n",
        "IMPORTANT FORMAT:\n",
        "Respond with exactly 4 sections in this format:\n",
        "1. Food Recommendation:\n",
        "2. Physical Exercise:\n",
        "3. Sleep and Recovery:\n",
        "4. Overall Suggestion:\n",
        "\n",
        "Use bullet points with a), b), c), d) format for each section.\n",
        "End with \"### End ###\"\n",
        "<|/system|>\n",
        "\n",
        "<|user|>\n",
        "{context}\n",
        "\n",
        "Based on this analysis, provide comprehensive health recommendations for the upcoming week.\n",
        "<|/user|>\n",
        "\n",
        "<|assistant|>\n",
        "1. Food Recommendation:\n",
        "\"\"\"\n",
        "\n",
        "            print(f\"📝 Sending prompt to LLM (length: {len(prompt)} chars)\")\n",
        "\n",
        "            # Generate with your model\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1500)\n",
        "\n",
        "            # Move to same device as model\n",
        "            if next(self.model.parameters()).device != inputs['input_ids'].device:\n",
        "                inputs = {k: v.to(next(self.model.parameters()).device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=400,\n",
        "                    min_new_tokens=100,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id,\n",
        "                    repetition_penalty=1.1\n",
        "                )\n",
        "\n",
        "            # Decode response\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            response = response[len(prompt):].strip()\n",
        "\n",
        "            print(f\"✅ LLM generated {len(response)} characters\")\n",
        "            print(f\"📄 Response preview: {response[:200]}...\")\n",
        "\n",
        "            # Parse the response\n",
        "            parsed_recommendations = self._parse_llm_response(response)\n",
        "\n",
        "            # Add metadata\n",
        "            parsed_recommendations['llm_generated'] = True\n",
        "            parsed_recommendations['model_used'] = 'AnjaliNV/WellBeing_LLM'\n",
        "\n",
        "            return parsed_recommendations\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ LLM generation failed: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return self._generate_smart_fallback(analysis, week_data)\n",
        "\n",
        "    def _build_llm_context(self, analysis: Dict, week_data: Dict, profile) -> str:\n",
        "        \"\"\"Build comprehensive context for LLM\"\"\"\n",
        "\n",
        "        performance = analysis.get('performance', {})\n",
        "        priorities = analysis.get('priorities', [])\n",
        "\n",
        "        # Build context string\n",
        "        context_parts = []\n",
        "\n",
        "        # Overall performance\n",
        "        overall_score = performance.get('overall', {}).get('score', 0)\n",
        "        context_parts.append(f\"Overall Health Score: {overall_score:.1f}/100\")\n",
        "\n",
        "        # Performance breakdown\n",
        "        perf_summary = []\n",
        "        for area, perf in performance.items():\n",
        "            if area != 'overall':\n",
        "                achievement = perf.get('achievement_percent', 0)\n",
        "                current = perf.get('current', 0)\n",
        "                target = perf.get('target', 0)\n",
        "                perf_summary.append(f\"{area}: {current}/{target} ({achievement:.1f}%)\")\n",
        "\n",
        "        context_parts.append(\"Performance: \" + \", \".join(perf_summary))\n",
        "\n",
        "        # Priority areas\n",
        "        if priorities:\n",
        "            priority_list = [p['area'] for p in priorities[:3]]\n",
        "            context_parts.append(f\"Priority areas: {', '.join(priority_list)}\")\n",
        "\n",
        "        # Food data analysis\n",
        "        food_data = week_data.get('food_data', {})\n",
        "        if food_data:\n",
        "            food_summary = []\n",
        "            targets = {\n",
        "                'dairy': 14, 'legumes': 5, 'meat': 7, 'fruits': 21,\n",
        "                'vegetables': 35, 'grains': 28, 'nuts_seeds': 7, 'water_glasses': 56\n",
        "            }\n",
        "\n",
        "            for category, target in targets.items():\n",
        "                if category in food_data:\n",
        "                    daily_values = food_data[category]\n",
        "                    current = sum(daily_values) if isinstance(daily_values, list) else daily_values\n",
        "                    percentage = (current / target) * 100 if target > 0 else 0\n",
        "                    food_summary.append(f\"{category}: {current}/{target} ({percentage:.0f}%)\")\n",
        "\n",
        "            context_parts.append(\"Food intake: \" + \", \".join(food_summary))\n",
        "\n",
        "        # User preferences\n",
        "        if hasattr(profile, 'preferences') and profile.preferences:\n",
        "            restrictions = profile.preferences.get('food_restrictions', [])\n",
        "            allergies = profile.preferences.get('food_allergies', [])\n",
        "            if restrictions:\n",
        "                context_parts.append(f\"Dietary restrictions: {', '.join(restrictions)}\")\n",
        "            if allergies:\n",
        "                context_parts.append(f\"Food allergies: {', '.join(allergies)}\")\n",
        "\n",
        "        # Add knowledge base context if available\n",
        "        if self.vectorstore:\n",
        "            try:\n",
        "                # Search for relevant information\n",
        "                query = f\"health recommendations {' '.join(priority_list[:2]) if priorities else 'wellness'}\"\n",
        "                docs = self.vectorstore.similarity_search(query, k=2)\n",
        "                if docs:\n",
        "                    context_parts.append(\"Relevant guidance: \" + docs[0].page_content[:300])\n",
        "            except:\n",
        "                pass  # If vector search fails, continue without it\n",
        "\n",
        "        return \". \".join(context_parts)\n",
        "\n",
        "    def _parse_llm_response(self, response: str) -> Dict:\n",
        "        \"\"\"Parse LLM response into structured recommendations\"\"\"\n",
        "\n",
        "        import re\n",
        "\n",
        "        recommendations = {\n",
        "            'food_recommendation': '',\n",
        "            'physical_exercise': '',\n",
        "            'sleep_and_recovery': '',\n",
        "            'overall_suggestion': ''\n",
        "        }\n",
        "\n",
        "        # Try to extract sections using regex\n",
        "        patterns = {\n",
        "            'food_recommendation': r'1\\.\\s*Food Recommendation[:\\s]*(.*?)(?=2\\.\\s*Physical Exercise|$)',\n",
        "            'physical_exercise': r'2\\.\\s*Physical Exercise[:\\s]*(.*?)(?=3\\.\\s*Sleep and Recovery|$)',\n",
        "            'sleep_and_recovery': r'3\\.\\s*Sleep and Recovery[:\\s]*(.*?)(?=4\\.\\s*Overall Suggestion|$)',\n",
        "            'overall_suggestion': r'4\\.\\s*Overall Suggestion[:\\s]*(.*?)(?=### End ###|$)'\n",
        "        }\n",
        "\n",
        "        for section, pattern in patterns.items():\n",
        "            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)\n",
        "            if match:\n",
        "                content = match.group(1).strip()\n",
        "                # Clean up content\n",
        "                content = re.sub(r'\\n+', '\\n', content)\n",
        "                content = content.strip()\n",
        "                recommendations[section] = content\n",
        "\n",
        "        # If parsing failed, try simpler approach\n",
        "        if not any(recommendations.values()):\n",
        "            # Split by numbers\n",
        "            parts = re.split(r'\\d+\\.\\s*(?:Food|Physical|Sleep|Overall)', response)\n",
        "            if len(parts) >= 4:\n",
        "                recommendations['food_recommendation'] = parts[1].strip() if len(parts) > 1 else ''\n",
        "                recommendations['physical_exercise'] = parts[2].strip() if len(parts) > 2 else ''\n",
        "                recommendations['sleep_and_recovery'] = parts[3].strip() if len(parts) > 3 else ''\n",
        "                recommendations['overall_suggestion'] = parts[4].strip() if len(parts) > 4 else ''\n",
        "\n",
        "        # Fallback: use entire response if still empty\n",
        "        if not any(recommendations.values()):\n",
        "            recommendations['overall_suggestion'] = response[:500] + \"...\" if len(response) > 500 else response\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def _generate_smart_fallback(self, analysis: Dict, week_data: Dict) -> Dict:\n",
        "        \"\"\"Smart fallback recommendations when LLM fails\"\"\"\n",
        "\n",
        "        return {\n",
        "            'food_recommendation': 'LLM unavailable - Focus on balanced nutrition with variety from all food groups.',\n",
        "            'physical_exercise': 'LLM unavailable - Continue regular exercise and aim for 150 zone minutes per week.',\n",
        "            'sleep_and_recovery': 'LLM unavailable - Maintain 7-9 hours of quality sleep nightly.',\n",
        "            'overall_suggestion': 'LLM unavailable - Keep up your healthy habits and focus on consistency.',\n",
        "            'llm_generated': False\n",
        "        }\n",
        "\n",
        "    def _create_week_summary(self, weekly_data, week_data: Dict) -> str:\n",
        "        \"\"\"Create week summary\"\"\"\n",
        "        summary = f\"Week of {weekly_data.week_start}:\\n\"\n",
        "        summary += f\"• Steps: {weekly_data.total_steps:,} ({weekly_data.avg_daily_steps:.0f}/day)\\n\"\n",
        "        summary += f\"• Zone Minutes: {weekly_data.zone_minutes}\\n\"\n",
        "        summary += f\"• Average Sleep: {weekly_data.avg_sleep:.1f} hours\\n\"\n",
        "        summary += f\"• Exercise Sessions: {len(weekly_data.exercise_sessions)}\\n\"\n",
        "        summary += f\"• Average Mood: {weekly_data.avg_mood:.1f}/10\\n\"\n",
        "\n",
        "        # Add LLM status\n",
        "        if self.model:\n",
        "            summary += \"• LLM Status: ✅ Active (using your fine-tuned model)\\n\"\n",
        "        else:\n",
        "            summary += \"• LLM Status: ❌ Unavailable (using rule-based recommendations)\\n\"\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def _create_error_response(self, user_id: str, week_data: Dict, error: str) -> Dict:\n",
        "        \"\"\"Create error response\"\"\"\n",
        "        return {\n",
        "            'user_id': user_id,\n",
        "            'week_start': week_data.get('week_start', 'unknown'),\n",
        "            'analysis': {'performance': {'overall': {'score': 0}}, 'priorities': []},\n",
        "            'recommendations': {\n",
        "                'food_recommendation': f'Error: {error}',\n",
        "                'physical_exercise': 'System error occurred.',\n",
        "                'sleep_and_recovery': 'Please try again.',\n",
        "                'overall_suggestion': 'Contact support if issue persists.'\n",
        "            },\n",
        "            'insights': ['System error during analysis'],\n",
        "            'week_summary': f'Error: {error}',\n",
        "            'llm_used': False\n",
        "        }\n",
        "\n",
        "    def provide_feedback(self, user_id: str, week_start: str, feedback: Dict):\n",
        "        \"\"\"Provide feedback\"\"\"\n",
        "        print(f\"✅ Feedback received for {user_id}\")\n",
        "\n",
        "    def get_user_progress(self, user_id: str, weeks: int = 8) -> Dict:\n",
        "        \"\"\"Get user progress\"\"\"\n",
        "        try:\n",
        "            progress_summary = self.data_manager.get_user_progress_summary(user_id)\n",
        "            recent_recommendations = self.data_manager.load_user_recommendations(user_id, weeks)\n",
        "\n",
        "            return {\n",
        "                'progress_summary': progress_summary,\n",
        "                'recent_recommendations': [rec.to_dict() for rec in recent_recommendations],\n",
        "                'trend_analysis': {'steps_trend': 'stable', 'zone_trend': 'stable', 'sleep_trend': 'stable'}\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'progress_summary': {'message': f'Error: {str(e)}'},\n",
        "                'recent_recommendations': [],\n",
        "                'trend_analysis': {}\n",
        "            }\n",
        "\n",
        "# ========================================================================\n",
        "# CREATE SYSTEM THAT ACTUALLY USES YOUR LLM\n",
        "# ========================================================================\n",
        "\n",
        "def create_llm_health_app():\n",
        "    \"\"\"Create health app that actually uses your LLM\"\"\"\n",
        "\n",
        "    print(\"🚀 Creating Health App with LLM Integration...\")\n",
        "\n",
        "    # Create system with LLM\n",
        "    health_system = WorkingLLMHealthSystem(\n",
        "        base_model_id=\"ContactDoctor/Bio-Medical-Llama-3-8B\",\n",
        "        adapter_path=\"AnjaliNV/WellBeing_LLM\",\n",
        "        vector_db_path=\"/content/drive/MyDrive/rag_index\",\n",
        "        data_dir=\"/content/health_data\"\n",
        "    )\n",
        "\n",
        "    # Create interface\n",
        "    interface = FixedHealthAnalyticsInterface(health_system)\n",
        "    demo = interface.create_simple_working_interface()\n",
        "\n",
        "    print(\"✅ LLM Health App created!\")\n",
        "\n",
        "    # Test LLM\n",
        "    if health_system.model:\n",
        "        print(\"🎉 Your LLM is loaded and ready!\")\n",
        "        print(\"✅ Recommendations will be generated by your fine-tuned model\")\n",
        "    else:\n",
        "        print(\"⚠️ LLM failed to load - will use smart fallbacks\")\n",
        "        print(\"🔧 Check GPU memory or try restarting runtime\")\n",
        "\n",
        "    return demo, health_system\n",
        "\n",
        "def test_llm_system():\n",
        "    \"\"\"Test the LLM system\"\"\"\n",
        "\n",
        "    print(\"🧪 Testing LLM System...\")\n",
        "\n",
        "    try:\n",
        "        demo, health_system = create_llm_health_app()\n",
        "\n",
        "        # Test with data\n",
        "        test_data = {\n",
        "            'week_start': '2024-01-15',\n",
        "            'total_steps': 55000,  # Below target\n",
        "            'zone_minutes': 90,    # Below target\n",
        "            'sleep_hours': [6.5, 6.0, 7.5, 6.8, 6.2, 8.0, 7.0],\n",
        "            'mood_scores': [6, 5, 7, 6, 5, 7, 6],\n",
        "            'exercise_sessions': [{'type': 'test', 'duration': 30}],\n",
        "            'food_data': {\n",
        "                'dairy': [1, 2, 1, 2, 1, 2, 1],      # 10/14 - low\n",
        "                'fruits': [2, 2, 3, 2, 2, 3, 2],     # 16/21 - low\n",
        "                'vegetables': [3, 4, 3, 4, 3, 4, 3], # 24/35 - low\n",
        "                'water_glasses': [6, 7, 6, 7, 6, 7, 6] # 45/56 - low\n",
        "            }\n",
        "        }\n",
        "\n",
        "        result = health_system.process_weekly_data(\"llm_test_user\", test_data, {\n",
        "            'daily_steps': 10000, 'weekly_zone_minutes': 150,\n",
        "            'daily_sleep_hours': 8, 'weekly_exercise_sessions': 3\n",
        "        })\n",
        "\n",
        "        print(f\"✅ Test completed!\")\n",
        "        print(f\"🎯 Health Score: {result['analysis']['performance']['overall']['score']:.1f}/100\")\n",
        "        print(f\"🤖 LLM Used: {'✅ Yes' if result.get('llm_used') else '❌ No'}\")\n",
        "\n",
        "        if result.get('llm_used'):\n",
        "            print(\"🎉 SUCCESS: Your LLM generated the recommendations!\")\n",
        "        else:\n",
        "            print(\"⚠️ LLM not used - check model loading\")\n",
        "\n",
        "        return demo, health_system\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Test failed: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n",
        "\n",
        "print(\"\"\"\n",
        "🤖 LLM-POWERED HEALTH SYSTEM READY!\n",
        "\n",
        "This version actually loads and uses your fine-tuned model for recommendations.\n",
        "\n",
        "To use:\n",
        ">>> demo, health_system = create_llm_health_app()\n",
        ">>> demo.launch(share=True)\n",
        "\n",
        "To test LLM first:\n",
        ">>> demo, health_system = test_llm_system()\n",
        ">>> demo.launch(share=True)\n",
        "\n",
        "Features:\n",
        "✅ Loads your actual fine-tuned model (AnjaliNV/WellBeing_LLM)\n",
        "✅ Uses advanced memory optimization for GPU constraints\n",
        "✅ Generates recommendations with your LLM\n",
        "✅ Falls back gracefully if LLM fails\n",
        "✅ Shows LLM status in results\n",
        "✅ Handles food categories in LLM context\n",
        "✅ Proper prompt formatting for your model\n",
        "\n",
        "The system will tell you if LLM is working:\n",
        "- \"LLM Status: ✅ Active\" = Your model is generating recommendations\n",
        "- \"LLM Status: ❌ Unavailable\" = Using fallback recommendations\n",
        "\n",
        "Run the test to see if your LLM loads properly! 🚀\n",
        "\"\"\")\n",
        "\n",
        "# Uncomment to test:\n",
        "# demo, health_system = test_llm_system()\n",
        "# demo.launch(share=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QdjMVziJ0TzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and launch immediately\n",
        "demo, health_system = create_llm_health_app()\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "43mfkcET06HY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "https://github.com/AnjaliVaghjiani/Thesis/blob/main/WellBeing.ipynb",
      "authorship_tag": "ABX9TyNk6EIeagR95wcojUCgCClR"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}