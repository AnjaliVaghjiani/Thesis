{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ2IZnY0spOf",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQs-1g9pO2vc"
      },
      "source": [
        "# ContactDoctor Bio Medical LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zBB_GqePIUt",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8Eu1zo5Q2Z54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "-B9Uv4jEPLeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data from the file to train the model"
      ],
      "metadata": {
        "id": "B3npJmgHqYnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "data = [  ]\n",
        "dataset = Dataset.from_list(data)"
      ],
      "metadata": {
        "id": "rLUmuiVkqYD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset\n",
        "from datasets import load_dataset\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "# === Step 1: Model & Tokenizer Setup ===\n",
        "model_id = \"AnjaliNV/WellBeing_LLM\"  # or base model like \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "print(\"üîÑ Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Required for some models\n",
        "\n",
        "print(\"üìÇ Loading local dataset...\")\n",
        "# dataset = load_dataset(\"json\", data_files=\"/content/fitness-chat-prompt-completion-dataset.json\")  # Update path\n",
        "# Using the dataset created in the previous cell\n",
        "\n",
        "\n",
        "# Tokenize function\n",
        "# def preprocess(batch):\n",
        "#     texts = [f\"### Question: {p}\\n### Answer: {c}\" for p, c in zip(batch[\"instruction\"], batch[\"output\"])]\n",
        "#     tokens = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=512)\n",
        "#     tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "#     return tokens\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Concatenate prompt + completion as text to train on\n",
        "    texts = [p + c for p, c in zip(examples[\"prompt\"], examples[\"completion\"])]\n",
        "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"üîÑ Tokenizing dataset...\")\n",
        "# Process without batching to avoid potential issues with padding/truncation in batches\n",
        "# tokenized_dataset = dataset[\"train\"].map(preprocess, batched=True)\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "\n",
        "# === Step 2: Load Quantized Base Model ===\n",
        "print(\"üîÑ Loading base model...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# === Step 3: Inject LoRA ===\n",
        "print(\"üß† Adding LoRA adapters...\")\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# === Step 4: Set Up Trainer ===\n",
        "print(\"‚öôÔ∏è Preparing trainer...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./WellBeing_LLM_finetuned\",\n",
        "    per_device_train_batch_size=1,      # lowered batch size\n",
        "    gradient_accumulation_steps=8,      # to keep effective batch size\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=1,\n",
        "    learning_rate=3e-4,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# === Step 5: Train ===\n",
        "print(\"üöÄ Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# === Step 6: Save Final Model + Tokenizer ===\n",
        "print(\"üíæ Saving the fine-tuned model...\")\n",
        "trainer.save_model(\"./WellBeing_LLM_finetuned\")\n",
        "tokenizer.save_pretrained(\"./WellBeing_LLM_finetuned\")"
      ],
      "metadata": {
        "id": "k0AWAIyXN0uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Optional: If running in Colab/Jupyter\n",
        "# notebook_login()\n",
        "\n",
        "repo_name = \"AnjaliNV/WellBeing_LLM\"  # Choose a name\n",
        "\n",
        "# Push the LoRA adapter\n",
        "model.push_to_hub(repo_name)\n",
        "tokenizer.push_to_hub(repo_name)\n"
      ],
      "metadata": {
        "id": "wf10JbVZ6CLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "base_model_id = \"ContactDoctor/Bio-Medical-Llama-3-8B\"\n",
        "adapter_path = \"AnjaliNV/WellBeing_LLM\"\n",
        "\n",
        "# Optional: Enable 4-bit quantization to reduce memory usage\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
        "\n",
        "# Load model with quantization and low memory footprint\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load the LoRA adapter (AFTER the base model is on device)\n",
        "model = PeftModel.from_pretrained(model, adapter_path)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "B-kSkpUK_dDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG fine tuning"
      ],
      "metadata": {
        "id": "jJQyIxhw-gyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain faiss-cpu sentence-transformers transformers\n"
      ],
      "metadata": {
        "id": "PaVHh0jR-kvv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bQip4YCFKa4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3t4w4p6nK9CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "vectorstore = FAISS.load_local(\n",
        "    \"/content/drive/MyDrive/rag_index\",\n",
        "    embeddings=embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "DganfMFKEXxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_split_document(filepath):\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    import PyPDF2\n",
        "\n",
        "    text = \"\"\n",
        "\n",
        "    if filepath.endswith(\".txt\"):\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "    elif filepath.endswith(\".pdf\"):\n",
        "        with open(filepath, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "    # Split text into chunks\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "new_chunks = load_and_split_document(\"/content/physical_exercise2.pdf\")  # or .txt\n",
        "vectorstore.add_texts(new_chunks)\n",
        "vectorstore.save_local(\"/content/drive/MyDrive/rag_index\")\n",
        "print(\"Vectorstore updated and saved.\")\n"
      ],
      "metadata": {
        "id": "wDobS1ltkYNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "import re\n",
        "\n",
        "def chat_rag_interface(message, history):\n",
        "    # Step 1: Retrieve relevant context chunks for the user question\n",
        "    retrieved_chunks = vectorstore.similarity_search(message, k=3)\n",
        "\n",
        "    # Step 2: Combine chunks into one context string with better formatting\n",
        "    context = \"\\n---\\n\".join([doc.page_content for doc in retrieved_chunks])\n",
        "\n",
        "    # Step 3: Improved prompt template with clearer instructions\n",
        "    prompt_template = f\"\"\"<|system|>\n",
        "You are a wellbeing expert. You MUST follow the exact format shown in the example below. Do NOT deviate from this structure.\n",
        "\n",
        "IMPORTANT RULES:\n",
        "1. Always respond with exactly 4 sections: Food Recommendation, Physical Exercise, Sleep and Recovery, Overall Suggestion\n",
        "2. Use bullet points with a), b), c), d) format as shown\n",
        "3. Base recommendations on the Zone Minutes logic provided\n",
        "4. Keep responses factual and avoid making up specific numbers or data not provided\n",
        "5. End your response with \"### End ###\"\n",
        "\n",
        "Zone Minutes Guidelines:\n",
        "- Zone Minutes < 90: Recommend starting with low-impact activities\n",
        "- Zone Minutes 90‚Äì150: Encourage maintaining current activity with gradual progression\n",
        "- Zone Minutes > 150: Recommend advanced workouts and advise on recovery\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "EXAMPLE FORMAT (FOLLOW EXACTLY):\n",
        "User Data: Total Steps: 100,000, Average Zone Minutes: 160, Average Heart Rate: 60 bpm, Food Consumption: healthy, Sleep: 7.5 hrs average, Physical Activities: running, gym\n",
        "\n",
        "Response:\n",
        "1. Food Recommendation:\n",
        "   Your food intake is already healthy, try different varieties of food\n",
        "   a) Protein Intake: Include lean meats, fish, tofu, or legumes to support muscle repair\n",
        "   b) Healthy Fats: Add avocados, olive oil, almonds, walnuts, and chia seeds\n",
        "   c) Vegetable Variety: Eat spinach, broccoli, carrots, and zucchini for essential vitamins\n",
        "   d) Hydration: Aim for 3‚Äì4 liters of water per day to stay well-hydrated\n",
        "\n",
        "2. Physical Exercise:\n",
        "   You have good cardiovascular fitness from running. Along with walking you can include:\n",
        "   a) Strength training: squats, deadlifts, bench press, and overhead press\n",
        "   b) Yoga or Pilates for flexibility and core strength\n",
        "   c) Outdoor Activities/Adventure Sports: Cycling, tracking, swimming\n",
        "\n",
        "3. Sleep and Recovery:\n",
        "   Aim for 7‚Äì8 hours of sleep. Avoid screens before bed and follow a consistent schedule.\n",
        "\n",
        "4. Overall Suggestion:\n",
        "   Great job! Keep up the consistency. You could also explore group fitness or meditation for balance.\n",
        "\n",
        "### End ###\n",
        "<|/system|>\n",
        "\n",
        "<|user|>\n",
        "User Data: {message}\n",
        "<|/user|>\n",
        "\n",
        "<|assistant|>\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "    # Step 4: Improved tokenization and generation parameters\n",
        "    inputs = tokenizer(prompt_template, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
        "\n",
        "    # Better generation parameters to reduce hallucination\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=400,  # Reduced to prevent over-generation\n",
        "            min_new_tokens=50,   # Ensure minimum response length\n",
        "            temperature=0.3,     # Lower temperature for more focused responses\n",
        "            top_p=0.85,          # Slightly lower for better coherence\n",
        "            top_k=40,            # Add top_k sampling\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.1,  # Reduced repetition penalty\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=[\n",
        "                tokenizer.encode(\"### End ###\")[0] if \"### End ###\" in tokenizer.get_vocab() else tokenizer.eos_token_id,\n",
        "                tokenizer.eos_token_id\n",
        "            ],\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=3  # Prevent repetitive phrases\n",
        "        )\n",
        "\n",
        "    # Step 5: Better response extraction and cleaning\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the assistant's response\n",
        "    if \"<|assistant|>\" in response:\n",
        "        generated_text = response.split(\"<|assistant|>\")[-1].strip()\n",
        "    else:\n",
        "        # Fallback: remove the prompt template\n",
        "        generated_text = response[len(prompt_template):].strip()\n",
        "\n",
        "    # Clean up the response\n",
        "    generated_text = clean_response(generated_text)\n",
        "\n",
        "    # Validate response format\n",
        "    if not validate_response_format(generated_text):\n",
        "        # If format is invalid, try to fix it or return a structured fallback\n",
        "        generated_text = fix_response_format(generated_text, message)\n",
        "\n",
        "    # Optional: Save chat history\n",
        "    with open(\"chat_history.txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"User: {message}\\n\")\n",
        "        f.write(f\"Model: {generated_text}\\n\\n\")\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "def clean_response(text):\n",
        "    \"\"\"Clean and format the model response\"\"\"\n",
        "    # Remove any remaining prompt artifacts\n",
        "    text = re.sub(r'<\\|.*?\\|>', '', text)\n",
        "    text = re.sub(r'User Data:.*?Response:', '', text, flags=re.DOTALL)\n",
        "\n",
        "    # Ensure proper formatting\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove multiple consecutive newlines\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def validate_response_format(text):\n",
        "    \"\"\"Check if response follows the required format\"\"\"\n",
        "    required_sections = [\n",
        "        \"1. Food Recommendation\",\n",
        "        \"2. Physical Exercise\",\n",
        "        \"3. Sleep and Recovery\",\n",
        "        \"4. Overall Suggestion\"\n",
        "    ]\n",
        "\n",
        "    for section in required_sections:\n",
        "        if section not in text:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def fix_response_format(text, user_message):\n",
        "    \"\"\"Attempt to fix malformed responses or provide fallback\"\"\"\n",
        "    # Extract zone minutes from user message if possible\n",
        "    zone_minutes_match = re.search(r'zone minutes?:?\\s*(\\d+)', user_message.lower())\n",
        "    zone_minutes = int(zone_minutes_match.group(1)) if zone_minutes_match else 100\n",
        "\n",
        "    # Create a properly formatted fallback response\n",
        "    if zone_minutes < 90:\n",
        "        exercise_rec = \"Start with low-impact activities like walking, light stretching, and basic bodyweight exercises\"\n",
        "    elif zone_minutes <= 150:\n",
        "        exercise_rec = \"Maintain your current activity level with gradual progression in intensity and duration\"\n",
        "    else:\n",
        "        exercise_rec = \"Focus on advanced workouts while prioritizing recovery and rest days\"\n",
        "\n",
        "    fallback_response = f\"\"\"1. Food Recommendation:\n",
        "   Focus on a balanced diet to support your fitness goals\n",
        "   a) Protein Intake: Include lean proteins to support muscle recovery\n",
        "   b) Complex Carbohydrates: Add whole grains and vegetables for sustained energy\n",
        "   c) Healthy Fats: Include nuts, seeds, and olive oil for overall health\n",
        "   d) Hydration: Maintain adequate water intake throughout the day\n",
        "\n",
        "2. Physical Exercise:\n",
        "   {exercise_rec}\n",
        "   a) Cardiovascular Training: Include activities that elevate your heart rate\n",
        "   b) Strength Training: Add resistance exercises for muscle development\n",
        "   c) Flexibility Work: Include stretching or yoga for mobility\n",
        "\n",
        "3. Sleep and Recovery:\n",
        "   Prioritize 7-9 hours of quality sleep each night for optimal recovery\n",
        "\n",
        "4. Overall Suggestion:\n",
        "   Maintain consistency in your wellness routine and listen to your body's needs\n",
        "\n",
        "### End ###\"\"\"\n",
        "\n",
        "    return fallback_response"
      ],
      "metadata": {
        "id": "NXrD-zXLxxvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(\n",
        "    fn=chat_rag_interface,\n",
        "    title=\"Well Being Advicer\",\n",
        "    description=\"Receive holistic health advice grounded in domain knowledge.\",\n",
        ").queue().launch(debug=True, share=True)\n"
      ],
      "metadata": {
        "id": "jaP6Sr2aFr4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"WellBeing_LLM\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"WellBeing_LLM\")\n",
        "\n",
        "# Push to HF Hub\n",
        "model.push_to_hub(\"WellBeing_LLM\")        # this creates a repo under your username\n",
        "tokenizer.push_to_hub(\"WellBeing_LLM\")\n"
      ],
      "metadata": {
        "id": "1bef4WCRaPI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data structure and Memory System"
      ],
      "metadata": {
        "id": "LV53MvRSxp2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================================\n",
        "# STEP 1: Data Structures and Memory System\n",
        "# ========================================================================\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Optional\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class UserProfile:\n",
        "    \"\"\"Store user's basic info, targets, and preferences\"\"\"\n",
        "\n",
        "    def __init__(self, user_id: str):\n",
        "        self.user_id = user_id\n",
        "        self.created_date = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "        # Default targets (can be customized per user)\n",
        "        self.targets = {\n",
        "            'daily_steps': 10000,\n",
        "            'weekly_zone_minutes': 150,\n",
        "            'daily_sleep_hours': 8,\n",
        "            'weekly_exercise_sessions': 3,\n",
        "            'daily_calories': 2000\n",
        "        }\n",
        "\n",
        "        # User preferences and constraints\n",
        "        self.preferences = {\n",
        "            'preferred_exercises': [],\n",
        "            'food_restrictions': [],\n",
        "            'schedule_constraints': {},\n",
        "            'health_conditions': []\n",
        "        }\n",
        "\n",
        "        # Learning data\n",
        "        self.response_patterns = {\n",
        "            'follows_exercise_recs': 0.5,  # How often they follow exercise advice\n",
        "            'follows_nutrition_recs': 0.5,\n",
        "            'follows_sleep_recs': 0.5,\n",
        "            'preferred_rec_types': []\n",
        "        }\n",
        "\n",
        "    def update_targets(self, new_targets: Dict):\n",
        "        \"\"\"Update user targets\"\"\"\n",
        "        self.targets.update(new_targets)\n",
        "\n",
        "    def update_preferences(self, new_prefs: Dict):\n",
        "        \"\"\"Update user preferences\"\"\"\n",
        "        self.preferences.update(new_prefs)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            'user_id': self.user_id,\n",
        "            'created_date': self.created_date,\n",
        "            'targets': self.targets,\n",
        "            'preferences': self.preferences,\n",
        "            'response_patterns': self.response_patterns\n",
        "        }\n",
        "\n",
        "class WeeklyHealthData:\n",
        "    \"\"\"Store one week's health data\"\"\"\n",
        "\n",
        "    def __init__(self, user_id: str, week_start: str, data: Dict):\n",
        "        self.user_id = user_id\n",
        "        self.week_start = week_start\n",
        "        self.week_number = self._calculate_week_number(week_start)\n",
        "\n",
        "        # Raw weekly data\n",
        "        self.total_steps = data.get('total_steps', 0)\n",
        "        self.zone_minutes = data.get('zone_minutes', 0)\n",
        "        self.sleep_hours = data.get('sleep_hours', [])  # Daily values\n",
        "        self.exercise_sessions = data.get('exercise_sessions', [])\n",
        "        self.mood_scores = data.get('mood_scores', [])\n",
        "        self.food_data = data.get('food_data', [])\n",
        "\n",
        "        # Calculated metrics\n",
        "        self.avg_daily_steps = self.total_steps / 7\n",
        "        self.avg_sleep = np.mean(self.sleep_hours) if self.sleep_hours else 0\n",
        "        self.avg_mood = np.mean(self.mood_scores) if self.mood_scores else 0\n",
        "        self.total_calories = sum([food.get('calories', 0) for food in self.food_data])\n",
        "        self.avg_daily_calories = self.total_calories / 7\n",
        "\n",
        "        # Timestamp\n",
        "        self.created_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    def _calculate_week_number(self, week_start: str) -> int:\n",
        "        \"\"\"Calculate week number since user started\"\"\"\n",
        "        week_date = datetime.strptime(week_start, '%Y-%m-%d')\n",
        "        start_of_year = datetime(week_date.year, 1, 1)\n",
        "        return ((week_date - start_of_year).days // 7) + 1\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            'user_id': self.user_id,\n",
        "            'week_start': self.week_start,\n",
        "            'week_number': self.week_number,\n",
        "            'total_steps': self.total_steps,\n",
        "            'zone_minutes': self.zone_minutes,\n",
        "            'sleep_hours': self.sleep_hours,\n",
        "            'exercise_sessions': self.exercise_sessions,\n",
        "            'mood_scores': self.mood_scores,\n",
        "            'food_data': self.food_data,\n",
        "            'avg_daily_steps': self.avg_daily_steps,\n",
        "            'avg_sleep': self.avg_sleep,\n",
        "            'avg_mood': self.avg_mood,\n",
        "            'total_calories': self.total_calories,\n",
        "            'avg_daily_calories': self.avg_daily_calories,\n",
        "            'created_at': self.created_at\n",
        "        }\n",
        "\n",
        "class WeeklyRecommendations:\n",
        "    \"\"\"Store recommendations given for a specific week\"\"\"\n",
        "\n",
        "    def __init__(self, user_id: str, week_start: str, recommendations: Dict):\n",
        "        self.user_id = user_id\n",
        "        self.week_start = week_start\n",
        "        self.recommendations = recommendations\n",
        "        self.created_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        # Track user response to recommendations\n",
        "        self.user_feedback = {\n",
        "            'followed_exercise': None,\n",
        "            'followed_nutrition': None,\n",
        "            'followed_sleep': None,\n",
        "            'difficulty_level': None,  # 1-5 scale\n",
        "            'effectiveness': None,     # 1-5 scale\n",
        "            'notes': \"\"\n",
        "        }\n",
        "\n",
        "    def update_feedback(self, feedback: Dict):\n",
        "        \"\"\"Update user feedback on recommendations\"\"\"\n",
        "        self.user_feedback.update(feedback)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            'user_id': self.user_id,\n",
        "            'week_start': self.week_start,\n",
        "            'recommendations': self.recommendations,\n",
        "            'user_feedback': self.user_feedback,\n",
        "            'created_at': self.created_at\n",
        "        }\n",
        "\n",
        "class HealthDataManager:\n",
        "    \"\"\"Manage all user health data and provide persistence\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir: str = \"health_data\"):\n",
        "        self.data_dir = data_dir\n",
        "        self._ensure_directories()\n",
        "\n",
        "    def _ensure_directories(self):\n",
        "        \"\"\"Create necessary directories\"\"\"\n",
        "        os.makedirs(f\"{self.data_dir}/profiles\", exist_ok=True)\n",
        "        os.makedirs(f\"{self.data_dir}/weekly_data\", exist_ok=True)\n",
        "        os.makedirs(f\"{self.data_dir}/recommendations\", exist_ok=True)\n",
        "\n",
        "    def save_user_profile(self, profile: UserProfile):\n",
        "        \"\"\"Save user profile to file\"\"\"\n",
        "        file_path = f\"{self.data_dir}/profiles/{profile.user_id}.json\"\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(profile.to_dict(), f, indent=2)\n",
        "\n",
        "    def load_user_profile(self, user_id: str) -> Optional[UserProfile]:\n",
        "        \"\"\"Load user profile from file\"\"\"\n",
        "        file_path = f\"{self.data_dir}/profiles/{user_id}.json\"\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                profile = UserProfile(user_id)\n",
        "                profile.targets = data['targets']\n",
        "                profile.preferences = data['preferences']\n",
        "                profile.response_patterns = data['response_patterns']\n",
        "                profile.created_date = data['created_date']\n",
        "                return profile\n",
        "        return None\n",
        "\n",
        "    def save_weekly_data(self, weekly_data: WeeklyHealthData):\n",
        "        \"\"\"Save weekly health data\"\"\"\n",
        "        user_dir = f\"{self.data_dir}/weekly_data/{weekly_data.user_id}\"\n",
        "        os.makedirs(user_dir, exist_ok=True)\n",
        "\n",
        "        file_path = f\"{user_dir}/{weekly_data.week_start}.json\"\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(weekly_data.to_dict(), f, indent=2)\n",
        "\n",
        "    def load_user_weekly_data(self, user_id: str, num_weeks: int = 4) -> List[WeeklyHealthData]:\n",
        "        \"\"\"Load recent weekly data for a user\"\"\"\n",
        "        user_dir = f\"{self.data_dir}/weekly_data/{user_id}\"\n",
        "        if not os.path.exists(user_dir):\n",
        "            return []\n",
        "\n",
        "        # Get all weekly data files\n",
        "        files = [f for f in os.listdir(user_dir) if f.endswith('.json')]\n",
        "        files.sort(reverse=True)  # Most recent first\n",
        "\n",
        "        weekly_data_list = []\n",
        "        for file in files[:num_weeks]:\n",
        "            file_path = f\"{user_dir}/{file}\"\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                weekly_data = WeeklyHealthData(\n",
        "                    data['user_id'],\n",
        "                    data['week_start'],\n",
        "                    data\n",
        "                )\n",
        "                weekly_data_list.append(weekly_data)\n",
        "\n",
        "        return weekly_data_list\n",
        "\n",
        "    def save_recommendations(self, recommendations: WeeklyRecommendations):\n",
        "        \"\"\"Save weekly recommendations\"\"\"\n",
        "        user_dir = f\"{self.data_dir}/recommendations/{recommendations.user_id}\"\n",
        "        os.makedirs(user_dir, exist_ok=True)\n",
        "\n",
        "        file_path = f\"{user_dir}/{recommendations.week_start}.json\"\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(recommendations.to_dict(), f, indent=2)\n",
        "\n",
        "    def load_user_recommendations(self, user_id: str, num_weeks: int = 4) -> List[WeeklyRecommendations]:\n",
        "        \"\"\"Load recent recommendations for a user\"\"\"\n",
        "        user_dir = f\"{self.data_dir}/recommendations/{user_id}\"\n",
        "        if not os.path.exists(user_dir):\n",
        "            return []\n",
        "\n",
        "        files = [f for f in os.listdir(user_dir) if f.endswith('.json')]\n",
        "        files.sort(reverse=True)\n",
        "\n",
        "        rec_list = []\n",
        "        for file in files[:num_weeks]:\n",
        "            file_path = f\"{user_dir}/{file}\"\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                rec = WeeklyRecommendations(\n",
        "                    data['user_id'],\n",
        "                    data['week_start'],\n",
        "                    data['recommendations']\n",
        "                )\n",
        "                rec.user_feedback = data['user_feedback']\n",
        "                rec.created_at = data['created_at']\n",
        "                rec_list.append(rec)\n",
        "\n",
        "        return rec_list\n",
        "\n",
        "    def get_user_progress_summary(self, user_id: str) -> Dict:\n",
        "        \"\"\"Get overall progress summary for a user\"\"\"\n",
        "        weekly_data = self.load_user_weekly_data(user_id, num_weeks=8)\n",
        "\n",
        "        if not weekly_data:\n",
        "            return {\"message\": \"No data available\"}\n",
        "\n",
        "        # Calculate trends\n",
        "        weeks = len(weekly_data)\n",
        "\n",
        "        # Steps trend\n",
        "        steps_trend = [w.avg_daily_steps for w in reversed(weekly_data)]\n",
        "\n",
        "        # Sleep trend\n",
        "        sleep_trend = [w.avg_sleep for w in reversed(weekly_data)]\n",
        "\n",
        "        # Zone minutes trend\n",
        "        zone_trend = [w.zone_minutes for w in reversed(weekly_data)]\n",
        "\n",
        "        # Mood trend\n",
        "        mood_trend = [w.avg_mood for w in reversed(weekly_data)]\n",
        "\n",
        "        return {\n",
        "            'weeks_tracked': weeks,\n",
        "            'steps_trend': steps_trend,\n",
        "            'sleep_trend': sleep_trend,\n",
        "            'zone_trend': zone_trend,\n",
        "            'mood_trend': mood_trend,\n",
        "            'latest_week': weekly_data[0].to_dict() if weekly_data else None\n",
        "        }\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 1 USAGE EXAMPLE\n",
        "# ========================================================================\n",
        "\n",
        "def example_usage():\n",
        "    \"\"\"Example of how to use the data management system\"\"\"\n",
        "\n",
        "    # Initialize data manager\n",
        "    data_manager = HealthDataManager()\n",
        "\n",
        "    # Create or load user profile\n",
        "    user_id = \"user_123\"\n",
        "    profile = data_manager.load_user_profile(user_id)\n",
        "\n",
        "    if not profile:\n",
        "        # Create new user\n",
        "        profile = UserProfile(user_id)\n",
        "        profile.update_targets({\n",
        "            'daily_steps': 12000,  # Custom target\n",
        "            'weekly_zone_minutes': 180\n",
        "        })\n",
        "        profile.update_preferences({\n",
        "            'preferred_exercises': ['running', 'yoga'],\n",
        "            'food_restrictions': ['gluten-free']\n",
        "        })\n",
        "        data_manager.save_user_profile(profile)\n",
        "        print(f\"Created new user profile for {user_id}\")\n",
        "    else:\n",
        "        print(f\"Loaded existing profile for {user_id}\")\n",
        "\n",
        "    # Example weekly data\n",
        "    week_data = {\n",
        "        'total_steps': 68000,\n",
        "        'zone_minutes': 140,\n",
        "        'sleep_hours': [7.5, 6.8, 8.2, 7.0, 6.5, 8.5, 7.8],\n",
        "        'exercise_sessions': [\n",
        "            {'type': 'running', 'duration': 30, 'date': '2024-01-15'},\n",
        "            {'type': 'yoga', 'duration': 45, 'date': '2024-01-17'}\n",
        "        ],\n",
        "        'mood_scores': [7, 6, 8, 7, 5, 8, 7],\n",
        "        'food_data': [\n",
        "            {'meal': 'breakfast', 'calories': 400, 'date': '2024-01-15'},\n",
        "            {'meal': 'lunch', 'calories': 600, 'date': '2024-01-15'},\n",
        "            # ... more food entries\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Save weekly data\n",
        "    weekly_health_data = WeeklyHealthData(user_id, \"2024-01-15\", week_data)\n",
        "    data_manager.save_weekly_data(weekly_health_data)\n",
        "    print(\"Saved weekly health data\")\n",
        "\n",
        "    # Example recommendations\n",
        "    recommendations = {\n",
        "        'exercise': \"Increase zone minutes by adding 2x20min cardio sessions\",\n",
        "        'nutrition': \"Add protein-rich snacks between meals\",\n",
        "        'sleep': \"Maintain current sleep schedule, it's working well\",\n",
        "        'overall': \"Focus on cardiovascular fitness this week\"\n",
        "    }\n",
        "\n",
        "    weekly_recs = WeeklyRecommendations(user_id, \"2024-01-15\", recommendations)\n",
        "    data_manager.save_recommendations(weekly_recs)\n",
        "    print(\"Saved weekly recommendations\")\n",
        "\n",
        "    # Get progress summary\n",
        "    progress = data_manager.get_user_progress_summary(user_id)\n",
        "    print(f\"Progress summary: {progress}\")\n",
        "\n",
        "    return data_manager\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the system\n",
        "    data_manager = example_usage()"
      ],
      "metadata": {
        "id": "E4m6zuEKxpOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analytics Engine"
      ],
      "metadata": {
        "id": "-N61Jyk9yXV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================================\n",
        "# STEP 2: Analytics Engine (Colab Version)\n",
        "# ========================================================================\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "from datetime import datetime\n",
        "import statistics\n",
        "\n",
        "class HealthAnalytics:\n",
        "    \"\"\"Core analytics engine for health data analysis\"\"\"\n",
        "\n",
        "    def __init__(self, data_manager):\n",
        "        self.data_manager = data_manager\n",
        "\n",
        "    def analyze_weekly_performance(self, user_id: str, current_week_data) -> Dict:\n",
        "        \"\"\"Comprehensive analysis of current week vs targets and trends\"\"\"\n",
        "\n",
        "        # Load user profile and historical data\n",
        "        profile = self.data_manager.load_user_profile(user_id)\n",
        "        historical_data = self.data_manager.load_user_weekly_data(user_id, num_weeks=4)\n",
        "\n",
        "        if not profile:\n",
        "            return {\"error\": \"User profile not found\"}\n",
        "\n",
        "        # Current week performance vs targets\n",
        "        performance = self._calculate_target_performance(current_week_data, profile.targets)\n",
        "\n",
        "        # Trend analysis\n",
        "        trends = self._analyze_trends(current_week_data, historical_data)\n",
        "\n",
        "        # Progress analysis\n",
        "        progress = self._analyze_progress(current_week_data, historical_data)\n",
        "\n",
        "        # Priority areas (what needs most attention)\n",
        "        priorities = self._identify_priority_areas(performance, trends)\n",
        "\n",
        "        return {\n",
        "            'user_id': user_id,\n",
        "            'week_start': current_week_data.week_start,\n",
        "            'performance': performance,\n",
        "            'trends': trends,\n",
        "            'progress': progress,\n",
        "            'priorities': priorities,\n",
        "            'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "\n",
        "    def _calculate_target_performance(self, current_data, targets: Dict) -> Dict:\n",
        "        \"\"\"Calculate performance against targets\"\"\"\n",
        "\n",
        "        performance = {}\n",
        "\n",
        "        # Steps performance\n",
        "        steps_achievement = (current_data.avg_daily_steps / targets['daily_steps']) * 100\n",
        "        performance['steps'] = {\n",
        "            'current': current_data.avg_daily_steps,\n",
        "            'target': targets['daily_steps'],\n",
        "            'achievement_percent': min(steps_achievement, 150),  # Cap at 150%\n",
        "            'status': self._get_performance_status(steps_achievement)\n",
        "        }\n",
        "\n",
        "        # Zone minutes performance\n",
        "        zone_achievement = (current_data.zone_minutes / targets['weekly_zone_minutes']) * 100\n",
        "        performance['zone_minutes'] = {\n",
        "            'current': current_data.zone_minutes,\n",
        "            'target': targets['weekly_zone_minutes'],\n",
        "            'achievement_percent': min(zone_achievement, 150),\n",
        "            'status': self._get_performance_status(zone_achievement)\n",
        "        }\n",
        "\n",
        "        # Sleep performance\n",
        "        sleep_achievement = (current_data.avg_sleep / targets['daily_sleep_hours']) * 100\n",
        "        performance['sleep'] = {\n",
        "            'current': current_data.avg_sleep,\n",
        "            'target': targets['daily_sleep_hours'],\n",
        "            'achievement_percent': min(sleep_achievement, 120),  # Sleep shouldn't be too much over\n",
        "            'status': self._get_performance_status(sleep_achievement)\n",
        "        }\n",
        "\n",
        "        # Exercise sessions\n",
        "        exercise_achievement = (len(current_data.exercise_sessions) / targets['weekly_exercise_sessions']) * 100\n",
        "        performance['exercise_sessions'] = {\n",
        "            'current': len(current_data.exercise_sessions),\n",
        "            'target': targets['weekly_exercise_sessions'],\n",
        "            'achievement_percent': min(exercise_achievement, 150),\n",
        "            'status': self._get_performance_status(exercise_achievement)\n",
        "        }\n",
        "\n",
        "        # Calories (if target is set)\n",
        "        if targets.get('daily_calories'):\n",
        "            calorie_achievement = (current_data.avg_daily_calories / targets['daily_calories']) * 100\n",
        "            performance['calories'] = {\n",
        "                'current': current_data.avg_daily_calories,\n",
        "                'target': targets['daily_calories'],\n",
        "                'achievement_percent': calorie_achievement,\n",
        "                'status': self._get_calorie_status(calorie_achievement)\n",
        "            }\n",
        "\n",
        "        # Overall performance score\n",
        "        scores = [perf['achievement_percent'] for perf in performance.values()]\n",
        "        performance['overall'] = {\n",
        "            'score': np.mean(scores),\n",
        "            'status': self._get_performance_status(np.mean(scores))\n",
        "        }\n",
        "\n",
        "        return performance\n",
        "\n",
        "    def _analyze_trends(self, current_data, historical_data: List) -> Dict:\n",
        "        \"\"\"Analyze trends over time\"\"\"\n",
        "\n",
        "        if len(historical_data) < 2:\n",
        "            return {\"message\": \"Insufficient data for trend analysis\"}\n",
        "\n",
        "        # Include current week in analysis\n",
        "        all_data = [current_data] + historical_data\n",
        "        all_data.sort(key=lambda x: x.week_start)  # Sort chronologically\n",
        "\n",
        "        trends = {}\n",
        "\n",
        "        # Steps trend\n",
        "        steps_data = [w.avg_daily_steps for w in all_data]\n",
        "        trends['steps'] = self._calculate_trend(steps_data, \"steps/day\")\n",
        "\n",
        "        # Zone minutes trend\n",
        "        zone_data = [w.zone_minutes for w in all_data]\n",
        "        trends['zone_minutes'] = self._calculate_trend(zone_data, \"minutes/week\")\n",
        "\n",
        "        # Sleep trend\n",
        "        sleep_data = [w.avg_sleep for w in all_data]\n",
        "        trends['sleep'] = self._calculate_trend(sleep_data, \"hours/night\")\n",
        "\n",
        "        # Mood trend\n",
        "        mood_data = [w.avg_mood for w in all_data if w.avg_mood > 0]\n",
        "        if mood_data:\n",
        "            trends['mood'] = self._calculate_trend(mood_data, \"mood score\")\n",
        "\n",
        "        return trends\n",
        "\n",
        "    def _analyze_progress(self, current_data, historical_data: List) -> Dict:\n",
        "        \"\"\"Analyze progress compared to previous weeks\"\"\"\n",
        "\n",
        "        if not historical_data:\n",
        "            return {\"message\": \"No historical data for comparison\"}\n",
        "\n",
        "        # Compare with last week\n",
        "        last_week = historical_data[0]  # Most recent week\n",
        "\n",
        "        progress = {}\n",
        "\n",
        "        # Steps progress\n",
        "        steps_change = current_data.avg_daily_steps - last_week.avg_daily_steps\n",
        "        progress['steps'] = {\n",
        "            'change': steps_change,\n",
        "            'percent_change': (steps_change / last_week.avg_daily_steps) * 100 if last_week.avg_daily_steps > 0 else 0,\n",
        "            'direction': 'improved' if steps_change > 0 else 'declined' if steps_change < 0 else 'maintained'\n",
        "        }\n",
        "\n",
        "        # Zone minutes progress\n",
        "        zone_change = current_data.zone_minutes - last_week.zone_minutes\n",
        "        progress['zone_minutes'] = {\n",
        "            'change': zone_change,\n",
        "            'percent_change': (zone_change / last_week.zone_minutes) * 100 if last_week.zone_minutes > 0 else 0,\n",
        "            'direction': 'improved' if zone_change > 0 else 'declined' if zone_change < 0 else 'maintained'\n",
        "        }\n",
        "\n",
        "        # Sleep progress\n",
        "        sleep_change = current_data.avg_sleep - last_week.avg_sleep\n",
        "        progress['sleep'] = {\n",
        "            'change': sleep_change,\n",
        "            'percent_change': (sleep_change / last_week.avg_sleep) * 100 if last_week.avg_sleep > 0 else 0,\n",
        "            'direction': 'improved' if sleep_change > 0 else 'declined' if sleep_change < 0 else 'maintained'\n",
        "        }\n",
        "\n",
        "        # Mood progress\n",
        "        if current_data.avg_mood > 0 and last_week.avg_mood > 0:\n",
        "            mood_change = current_data.avg_mood - last_week.avg_mood\n",
        "            progress['mood'] = {\n",
        "                'change': mood_change,\n",
        "                'percent_change': (mood_change / last_week.avg_mood) * 100,\n",
        "                'direction': 'improved' if mood_change > 0 else 'declined' if mood_change < 0 else 'maintained'\n",
        "            }\n",
        "\n",
        "        return progress\n",
        "\n",
        "    def _identify_priority_areas(self, performance: Dict, trends: Dict) -> List[Dict]:\n",
        "        \"\"\"Identify which areas need most attention\"\"\"\n",
        "\n",
        "        priorities = []\n",
        "\n",
        "        # Check each area\n",
        "        for area, perf in performance.items():\n",
        "            if area == 'overall':\n",
        "                continue\n",
        "\n",
        "            priority_score = 0\n",
        "            reasons = []\n",
        "\n",
        "            # Low performance\n",
        "            if perf['achievement_percent'] < 70:\n",
        "                priority_score += 3\n",
        "                reasons.append(f\"Below target ({perf['achievement_percent']:.1f}%)\")\n",
        "            elif perf['achievement_percent'] < 85:\n",
        "                priority_score += 2\n",
        "                reasons.append(f\"Slightly below target ({perf['achievement_percent']:.1f}%)\")\n",
        "\n",
        "            # Declining trend\n",
        "            if area in trends and trends[area].get('direction') == 'declining':\n",
        "                priority_score += 2\n",
        "                reasons.append(\"Declining trend\")\n",
        "\n",
        "            # Add to priorities if score is significant\n",
        "            if priority_score > 0:\n",
        "                priorities.append({\n",
        "                    'area': area,\n",
        "                    'priority_score': priority_score,\n",
        "                    'reasons': reasons,\n",
        "                    'current_performance': perf['achievement_percent']\n",
        "                })\n",
        "\n",
        "        # Sort by priority score (highest first)\n",
        "        priorities.sort(key=lambda x: x['priority_score'], reverse=True)\n",
        "\n",
        "        return priorities\n",
        "\n",
        "    def _calculate_trend(self, data: List[float], unit: str) -> Dict:\n",
        "        \"\"\"Calculate trend direction and magnitude\"\"\"\n",
        "\n",
        "        if len(data) < 2:\n",
        "            return {\"direction\": \"insufficient_data\"}\n",
        "\n",
        "        # Calculate linear trend\n",
        "        x = np.arange(len(data))\n",
        "        slope, intercept = np.polyfit(x, data, 1)\n",
        "\n",
        "        # Determine trend direction\n",
        "        if slope > 0.1:\n",
        "            direction = \"improving\"\n",
        "        elif slope < -0.1:\n",
        "            direction = \"declining\"\n",
        "        else:\n",
        "            direction = \"stable\"\n",
        "\n",
        "        # Calculate percentage change from first to last\n",
        "        if data[0] != 0:\n",
        "            percent_change = ((data[-1] - data[0]) / data[0]) * 100\n",
        "        else:\n",
        "            percent_change = 0\n",
        "\n",
        "        return {\n",
        "            'direction': direction,\n",
        "            'slope': slope,\n",
        "            'percent_change': percent_change,\n",
        "            'recent_value': data[-1],\n",
        "            'unit': unit,\n",
        "            'data_points': len(data)\n",
        "        }\n",
        "\n",
        "    def _get_performance_status(self, achievement_percent: float) -> str:\n",
        "        \"\"\"Get performance status based on achievement percentage\"\"\"\n",
        "        if achievement_percent >= 100:\n",
        "            return \"excellent\"\n",
        "        elif achievement_percent >= 85:\n",
        "            return \"good\"\n",
        "        elif achievement_percent >= 70:\n",
        "            return \"fair\"\n",
        "        else:\n",
        "            return \"needs_improvement\"\n",
        "\n",
        "    def _get_calorie_status(self, achievement_percent: float) -> str:\n",
        "        \"\"\"Get calorie status (different logic as too many calories is bad)\"\"\"\n",
        "        if 90 <= achievement_percent <= 110:\n",
        "            return \"excellent\"\n",
        "        elif 80 <= achievement_percent <= 120:\n",
        "            return \"good\"\n",
        "        elif 70 <= achievement_percent <= 130:\n",
        "            return \"fair\"\n",
        "        else:\n",
        "            return \"needs_adjustment\"\n",
        "\n",
        "    def generate_insights(self, analysis: Dict) -> List[str]:\n",
        "        \"\"\"Generate human-readable insights from analysis\"\"\"\n",
        "\n",
        "        insights = []\n",
        "\n",
        "        # Overall performance insight\n",
        "        overall_score = analysis['performance']['overall']['score']\n",
        "        if overall_score >= 90:\n",
        "            insights.append(\"üéâ Excellent overall performance this week!\")\n",
        "        elif overall_score >= 75:\n",
        "            insights.append(\"üëç Good overall performance with room for improvement\")\n",
        "        else:\n",
        "            insights.append(\"‚ö†Ô∏è Several areas need attention this week\")\n",
        "\n",
        "        # Priority area insights\n",
        "        priorities = analysis['priorities']\n",
        "        if priorities:\n",
        "            top_priority = priorities[0]\n",
        "            insights.append(f\"üéØ Top priority: {top_priority['area'].replace('_', ' ').title()}\")\n",
        "\n",
        "        # Trend insights - FIXED\n",
        "        trends = analysis.get('trends', {})\n",
        "        improving_areas = []\n",
        "        declining_areas = []\n",
        "\n",
        "        # Check if trends is a dictionary before iterating\n",
        "        if isinstance(trends, dict):\n",
        "            for area, trend in trends.items():\n",
        "                # Check if trend is a dictionary (not a string)\n",
        "                if isinstance(trend, dict):\n",
        "                    if trend.get('direction') == 'improving':\n",
        "                        improving_areas.append(area)\n",
        "                    elif trend.get('direction') == 'declining':\n",
        "                        declining_areas.append(area)\n",
        "                elif isinstance(trend, str):\n",
        "                    # Handle case where trend is just a string\n",
        "                    if trend == 'improving':\n",
        "                        improving_areas.append(area)\n",
        "                    elif trend == 'declining':\n",
        "                        declining_areas.append(area)\n",
        "\n",
        "        if improving_areas:\n",
        "            insights.append(f\"üìà Improving: {', '.join(improving_areas)}\")\n",
        "\n",
        "        if declining_areas:\n",
        "            insights.append(f\"üìâ Needs attention: {', '.join(declining_areas)}\")\n",
        "\n",
        "        # Progress insights - FIXED\n",
        "        progress = analysis.get('progress', {})\n",
        "        if isinstance(progress, dict):\n",
        "            for area, prog in progress.items():\n",
        "                if isinstance(prog, dict):\n",
        "                    if prog.get('direction') == 'improved' and prog.get('percent_change', 0) > 10:\n",
        "                        insights.append(f\"üöÄ Great improvement in {area}: +{prog['percent_change']:.1f}%\")\n",
        "\n",
        "        return insights\n",
        "\n",
        "# ========================================================================\n",
        "# CORRECTED USAGE EXAMPLE FOR COLAB\n",
        "# ========================================================================\n",
        "\n",
        "def example_analytics():\n",
        "    \"\"\"Example of using the analytics engine - CORRECTED FOR COLAB\"\"\"\n",
        "\n",
        "    # Use the data manager and classes from the previous cell (Step 1)\n",
        "    # No import needed since they're already defined in the notebook\n",
        "\n",
        "    data_manager = HealthDataManager()\n",
        "    analytics = HealthAnalytics(data_manager)\n",
        "\n",
        "    # Example current week data\n",
        "    current_week_data = WeeklyHealthData(\"user_123\", \"2024-01-22\", {\n",
        "        'total_steps': 72000,  # Improved from last week\n",
        "        'zone_minutes': 160,   # Good improvement\n",
        "        'sleep_hours': [7.8, 7.2, 8.1, 7.5, 7.0, 8.3, 7.9],\n",
        "        'exercise_sessions': [\n",
        "            {'type': 'running', 'duration': 35},\n",
        "            {'type': 'yoga', 'duration': 45},\n",
        "            {'type': 'cycling', 'duration': 40}\n",
        "        ],\n",
        "        'mood_scores': [8, 7, 8, 7, 6, 9, 8],\n",
        "        'food_data': []  # Simplified for example\n",
        "    })\n",
        "\n",
        "    # Perform analysis\n",
        "    analysis = analytics.analyze_weekly_performance(\"user_123\", current_week_data)\n",
        "\n",
        "    # Generate insights\n",
        "    insights = analytics.generate_insights(analysis)\n",
        "\n",
        "    print(\"=== WEEKLY HEALTH ANALYSIS ===\")\n",
        "    print(f\"User: {analysis['user_id']}\")\n",
        "    print(f\"Week: {analysis['week_start']}\")\n",
        "    print(f\"Overall Score: {analysis['performance']['overall']['score']:.1f}/100\")\n",
        "\n",
        "    print(\"\\n=== KEY INSIGHTS ===\")\n",
        "    for insight in insights:\n",
        "        print(insight)\n",
        "\n",
        "    print(\"\\n=== PERFORMANCE BREAKDOWN ===\")\n",
        "    for area, perf in analysis['performance'].items():\n",
        "        if area != 'overall':\n",
        "            print(f\"{area.replace('_', ' ').title()}: {perf['achievement_percent']:.1f}% ({perf['status']})\")\n",
        "\n",
        "    if analysis['priorities']:\n",
        "        print(\"\\n=== PRIORITY AREAS ===\")\n",
        "        for priority in analysis['priorities'][:3]:  # Top 3\n",
        "            print(f\"{priority['area'].replace('_', ' ').title()}: {', '.join(priority['reasons'])}\")\n",
        "\n",
        "    return analysis\n",
        "\n",
        "# Test the analytics (optional - run this to test)\n",
        "if __name__ == \"__main__\":\n",
        "    analysis = example_analytics()"
      ],
      "metadata": {
        "id": "gsqEhy7OyYq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================================\n",
        "# STEP 3: LLM Integration with Your Health Model\n",
        "# ========================================================================\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from langchain.vectorstores import FAISS\n",
        "# Updated import to fix deprecation warning\n",
        "try:\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "except ImportError:\n",
        "    from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import re\n",
        "from typing import Dict, List\n",
        "import json\n",
        "\n",
        "class HealthLLMProcessor:\n",
        "    \"\"\"Enhanced LLM processor for health analytics with memory and learning\"\"\"\n",
        "\n",
        "    def __init__(self, base_model_id: str, adapter_path: str, vector_db_path: str, data_manager):\n",
        "        self.data_manager = data_manager\n",
        "        self._load_model(base_model_id, adapter_path)\n",
        "        self._load_vector_db(vector_db_path)\n",
        "\n",
        "    def _load_model(self, base_model_id: str, adapter_path: str):\n",
        "        \"\"\"Load your fine-tuned model\"\"\"\n",
        "        print(\"Loading health LLM...\")\n",
        "\n",
        "        # Your exact configuration\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "        )\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Load model with quantization\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_id,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True,\n",
        "            quantization_config=bnb_config,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Load the LoRA adapter\n",
        "        self.model = PeftModel.from_pretrained(self.model, adapter_path)\n",
        "        self.model.eval()\n",
        "        print(\"‚úÖ Model loaded successfully\")\n",
        "\n",
        "    def _load_vector_db(self, vector_db_path: str):\n",
        "        \"\"\"Load your vector database\"\"\"\n",
        "        print(\"Loading vector database...\")\n",
        "        self.embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "        self.vectorstore = FAISS.load_local(\n",
        "            vector_db_path,\n",
        "            embeddings=self.embedding_model,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "        print(\"‚úÖ Vector database loaded successfully\")\n",
        "\n",
        "    def generate_weekly_recommendations(self, user_id: str, analysis_data: Dict) -> Dict:\n",
        "        \"\"\"Generate comprehensive weekly recommendations based on analysis\"\"\"\n",
        "\n",
        "        # Get user context\n",
        "        user_context = self._build_user_context(user_id)\n",
        "\n",
        "        # Get relevant knowledge from vector DB\n",
        "        knowledge_context = self._get_relevant_knowledge(analysis_data)\n",
        "\n",
        "        # Build prompt with all context\n",
        "        prompt = self._build_comprehensive_prompt(user_id, analysis_data, user_context, knowledge_context)\n",
        "\n",
        "        # Generate recommendations\n",
        "        raw_response = self._generate_response(prompt)\n",
        "\n",
        "        # Parse and structure the response\n",
        "        structured_recommendations = self._parse_recommendations(raw_response)\n",
        "\n",
        "        # Learn from this interaction\n",
        "        self._update_user_learning(user_id, analysis_data, structured_recommendations)\n",
        "\n",
        "        return structured_recommendations\n",
        "\n",
        "    def _build_user_context(self, user_id: str) -> str:\n",
        "        \"\"\"Build context about user's history and preferences\"\"\"\n",
        "\n",
        "        # Get user profile\n",
        "        profile = self.data_manager.load_user_profile(user_id)\n",
        "\n",
        "        # Get recent recommendations and feedback\n",
        "        recent_recs = self.data_manager.load_user_recommendations(user_id, num_weeks=2)\n",
        "\n",
        "        # Get historical data for patterns\n",
        "        historical_data = self.data_manager.load_user_weekly_data(user_id, num_weeks=4)\n",
        "\n",
        "        context = f\"USER PROFILE:\\n\"\n",
        "\n",
        "        if profile:\n",
        "            context += f\"- Targets: {profile.targets}\\n\"\n",
        "            context += f\"- Preferences: {profile.preferences}\\n\"\n",
        "            context += f\"- Response Patterns: {profile.response_patterns}\\n\"\n",
        "\n",
        "        if recent_recs:\n",
        "            context += f\"\\nRECENT RECOMMENDATIONS:\\n\"\n",
        "            for rec in recent_recs[:2]:  # Last 2 weeks\n",
        "                context += f\"Week {rec.week_start}:\\n\"\n",
        "                for area, recommendation in rec.recommendations.items():\n",
        "                    context += f\"  {area}: {recommendation}\\n\"\n",
        "                if rec.user_feedback.get('followed_exercise'):\n",
        "                    context += f\"  User followed exercise advice: {rec.user_feedback['followed_exercise']}\\n\"\n",
        "\n",
        "        if historical_data:\n",
        "            context += f\"\\nHISTORICAL PATTERNS:\\n\"\n",
        "            # Calculate what user typically achieves\n",
        "            avg_steps = sum([w.avg_daily_steps for w in historical_data]) / len(historical_data)\n",
        "            avg_zone = sum([w.zone_minutes for w in historical_data]) / len(historical_data)\n",
        "            context += f\"- Typical daily steps: {avg_steps:.0f}\\n\"\n",
        "            context += f\"- Typical zone minutes: {avg_zone:.0f}\\n\"\n",
        "\n",
        "        return context\n",
        "\n",
        "    def _get_relevant_knowledge(self, analysis_data: Dict) -> str:\n",
        "        \"\"\"Get relevant knowledge from vector database based on priority areas\"\"\"\n",
        "\n",
        "        priorities = analysis_data.get('priorities', [])\n",
        "\n",
        "        if not priorities:\n",
        "            # General health query\n",
        "            search_query = \"healthy lifestyle exercise nutrition sleep wellness\"\n",
        "        else:\n",
        "            # Build query based on priority areas\n",
        "            priority_areas = [p['area'] for p in priorities[:2]]  # Top 2 priorities\n",
        "            search_query = \" \".join(priority_areas) + \" improvement recommendations health\"\n",
        "\n",
        "        # Search vector database\n",
        "        relevant_docs = self.vectorstore.similarity_search(search_query, k=4)\n",
        "\n",
        "        # Combine relevant knowledge\n",
        "        knowledge = \"\\n---\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "        return knowledge[:2000]  # Limit context length\n",
        "\n",
        "    def _build_comprehensive_prompt(self, user_id: str, analysis_data: Dict, user_context: str, knowledge_context: str) -> str:\n",
        "        \"\"\"Build comprehensive prompt with all context\"\"\"\n",
        "\n",
        "        performance = analysis_data.get('performance', {})\n",
        "        priorities = analysis_data.get('priorities', [])\n",
        "        trends = analysis_data.get('trends', {})\n",
        "        progress = analysis_data.get('progress', {})\n",
        "\n",
        "        prompt = f\"\"\"<|system|>\n",
        "You are an expert health and wellness AI assistant. You provide personalized recommendations based on comprehensive health data analysis.\n",
        "\n",
        "IMPORTANT INSTRUCTIONS:\n",
        "1. Provide recommendations in EXACTLY 4 sections: Food Recommendation, Physical Exercise, Sleep and Recovery, Overall Suggestion\n",
        "2. Use bullet points with a), b), c), d) format\n",
        "3. Base recommendations on the user's specific data, priorities, and historical patterns\n",
        "4. Consider what has worked/not worked for this user before\n",
        "5. Make recommendations progressive and achievable\n",
        "6. End with \"### End ###\"\n",
        "\n",
        "{user_context}\n",
        "\n",
        "CURRENT WEEK ANALYSIS:\n",
        "Overall Performance Score: {performance.get('overall', {}).get('score', 0):.1f}/100\n",
        "\n",
        "PERFORMANCE BREAKDOWN:\n",
        "- Steps: {performance.get('steps', {}).get('achievement_percent', 0):.1f}% of target\n",
        "- Zone Minutes: {performance.get('zone_minutes', {}).get('achievement_percent', 0):.1f}% of target\n",
        "- Sleep: {performance.get('sleep', {}).get('achievement_percent', 0):.1f}% of target\n",
        "- Exercise Sessions: {performance.get('exercise_sessions', {}).get('achievement_percent', 0):.1f}% of target\n",
        "\n",
        "PRIORITY AREAS NEEDING ATTENTION:\n",
        "{self._format_priorities(priorities)}\n",
        "\n",
        "TRENDS:\n",
        "{self._format_trends(trends)}\n",
        "\n",
        "PROGRESS FROM LAST WEEK:\n",
        "{self._format_progress(progress)}\n",
        "\n",
        "KNOWLEDGE BASE CONTEXT:\n",
        "{knowledge_context}\n",
        "<|/system|>\n",
        "\n",
        "<|user|>\n",
        "Based on my comprehensive health analysis above, provide personalized recommendations for the upcoming week. Focus on my priority areas while building on what has worked for me before.\n",
        "<|/user|>\n",
        "\n",
        "<|assistant|>\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def _format_priorities(self, priorities: List[Dict]) -> str:\n",
        "        \"\"\"Format priority areas for prompt\"\"\"\n",
        "        if not priorities:\n",
        "            return \"No major areas of concern - maintain current performance\"\n",
        "\n",
        "        formatted = \"\"\n",
        "        for i, priority in enumerate(priorities[:3], 1):\n",
        "            formatted += f\"{i}. {priority['area'].replace('_', ' ').title()}: {', '.join(priority['reasons'])}\\n\"\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    def _format_trends(self, trends: Dict) -> str:\n",
        "        \"\"\"Format trends for prompt\"\"\"\n",
        "        if not trends:\n",
        "            return \"Insufficient data for trend analysis\"\n",
        "\n",
        "        formatted = \"\"\n",
        "        for area, trend in trends.items():\n",
        "            # Handle both string and dict trend values\n",
        "            if isinstance(trend, dict):\n",
        "                direction = trend.get('direction', 'stable')\n",
        "                change = trend.get('percent_change', 0)\n",
        "                formatted += f\"- {area.replace('_', ' ').title()}: {direction}\"\n",
        "                if change != 0:\n",
        "                    formatted += f\" ({change:+.1f}%)\"\n",
        "            elif isinstance(trend, str):\n",
        "                # Handle case where trend is just a string\n",
        "                if trend in ['improving', 'declining', 'stable', 'insufficient_data']:\n",
        "                    formatted += f\"- {area.replace('_', ' ').title()}: {trend}\"\n",
        "                else:\n",
        "                    formatted += f\"- {area.replace('_', ' ').title()}: insufficient data\"\n",
        "            else:\n",
        "                # Fallback for any other type\n",
        "                formatted += f\"- {area.replace('_', ' ').title()}: insufficient data\"\n",
        "            formatted += \"\\n\"\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    def _format_progress(self, progress: Dict) -> str:\n",
        "        \"\"\"Format progress for prompt\"\"\"\n",
        "        if not progress:\n",
        "            return \"No previous week for comparison\"\n",
        "\n",
        "        formatted = \"\"\n",
        "        for area, prog in progress.items():\n",
        "            # Handle both string and dict progress values\n",
        "            if isinstance(prog, dict):\n",
        "                direction = prog.get('direction', 'maintained')\n",
        "                change = prog.get('change', 0)\n",
        "                formatted += f\"- {area.replace('_', ' ').title()}: {direction}\"\n",
        "                if change != 0:\n",
        "                    formatted += f\" ({change:+.1f})\"\n",
        "            elif isinstance(prog, str):\n",
        "                # Handle case where progress is just a string\n",
        "                formatted += f\"- {area.replace('_', ' ').title()}: {prog}\"\n",
        "            else:\n",
        "                # Fallback\n",
        "                formatted += f\"- {area.replace('_', ' ').title()}: no data\"\n",
        "            formatted += \"\\n\"\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    def _generate_response(self, prompt: str) -> str:\n",
        "        \"\"\"Generate response using your model\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=500,\n",
        "                min_new_tokens=100,\n",
        "                temperature=0.3,\n",
        "                top_p=0.85,\n",
        "                top_k=40,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=[\n",
        "                    self.tokenizer.encode(\"### End ###\")[0] if \"### End ###\" in self.tokenizer.get_vocab() else self.tokenizer.eos_token_id,\n",
        "                    self.tokenizer.eos_token_id\n",
        "                ],\n",
        "                early_stopping=True,\n",
        "                no_repeat_ngram_size=3\n",
        "            )\n",
        "\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract only the assistant's response\n",
        "        if \"<|assistant|>\" in response:\n",
        "            generated_text = response.split(\"<|assistant|>\")[-1].strip()\n",
        "        else:\n",
        "            generated_text = response[len(prompt):].strip()\n",
        "\n",
        "        # Clean up the response\n",
        "        generated_text = re.sub(r'<\\|.*?\\|>', '', generated_text)\n",
        "        generated_text = generated_text.strip()\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "    def _parse_recommendations(self, raw_response: str) -> Dict:\n",
        "        \"\"\"Parse the LLM response into structured recommendations\"\"\"\n",
        "\n",
        "        sections = {\n",
        "            'food_recommendation': '',\n",
        "            'physical_exercise': '',\n",
        "            'sleep_and_recovery': '',\n",
        "            'overall_suggestion': '',\n",
        "            'raw_response': raw_response\n",
        "        }\n",
        "\n",
        "        # Extract each section\n",
        "        patterns = {\n",
        "            'food_recommendation': r'1\\.\\s*Food Recommendation[:\\s]*(.*?)(?=2\\.\\s*Physical Exercise|$)',\n",
        "            'physical_exercise': r'2\\.\\s*Physical Exercise[:\\s]*(.*?)(?=3\\.\\s*Sleep and Recovery|$)',\n",
        "            'sleep_and_recovery': r'3\\.\\s*Sleep and Recovery[:\\s]*(.*?)(?=4\\.\\s*Overall Suggestion|$)',\n",
        "            'overall_suggestion': r'4\\.\\s*Overall Suggestion[:\\s]*(.*?)(?=### End ###|$)'\n",
        "        }\n",
        "\n",
        "        for section, pattern in patterns.items():\n",
        "            match = re.search(pattern, raw_response, re.DOTALL | re.IGNORECASE)\n",
        "            if match:\n",
        "                sections[section] = match.group(1).strip()\n",
        "\n",
        "        return sections\n",
        "\n",
        "    def _update_user_learning(self, user_id: str, analysis_data: Dict, recommendations: Dict):\n",
        "        \"\"\"Update user learning patterns based on new recommendations\"\"\"\n",
        "\n",
        "        # This will help improve future recommendations\n",
        "        profile = self.data_manager.load_user_profile(user_id)\n",
        "\n",
        "        if profile:\n",
        "            # Track recommendation types given\n",
        "            priorities = analysis_data.get('priorities', [])\n",
        "            if priorities:\n",
        "                top_priority = priorities[0]['area']\n",
        "                if top_priority not in profile.response_patterns.get('areas_worked_on', []):\n",
        "                    if 'areas_worked_on' not in profile.response_patterns:\n",
        "                        profile.response_patterns['areas_worked_on'] = []\n",
        "                    profile.response_patterns['areas_worked_on'].append(top_priority)\n",
        "\n",
        "            # Save updated profile\n",
        "            self.data_manager.save_user_profile(profile)\n",
        "\n",
        "    def update_recommendation_feedback(self, user_id: str, week_start: str, feedback: Dict):\n",
        "        \"\"\"Update feedback on recommendations to improve future suggestions\"\"\"\n",
        "\n",
        "        # Load the recommendation\n",
        "        recommendations = self.data_manager.load_user_recommendations(user_id, num_weeks=4)\n",
        "\n",
        "        for rec in recommendations:\n",
        "            if rec.week_start == week_start:\n",
        "                rec.update_feedback(feedback)\n",
        "                self.data_manager.save_recommendations(rec)\n",
        "\n",
        "                # Update user profile learning patterns\n",
        "                profile = self.data_manager.load_user_profile(user_id)\n",
        "                if profile:\n",
        "                    # Update response patterns based on feedback\n",
        "                    if feedback.get('followed_exercise'):\n",
        "                        profile.response_patterns['follows_exercise_recs'] = min(1.0,\n",
        "                            profile.response_patterns.get('follows_exercise_recs', 0.5) + 0.1)\n",
        "\n",
        "                    if feedback.get('followed_nutrition'):\n",
        "                        profile.response_patterns['follows_nutrition_recs'] = min(1.0,\n",
        "                            profile.response_patterns.get('follows_nutrition_recs', 0.5) + 0.1)\n",
        "\n",
        "                    if feedback.get('followed_sleep'):\n",
        "                        profile.response_patterns['follows_sleep_recs'] = min(1.0,\n",
        "                            profile.response_patterns.get('follows_sleep_recs', 0.5) + 0.1)\n",
        "\n",
        "                    self.data_manager.save_user_profile(profile)\n",
        "                break\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 3 USAGE EXAMPLE (COLAB FIXED)\n",
        "# ========================================================================\n",
        "\n",
        "def example_llm_processing():\n",
        "    \"\"\"Example of using the enhanced LLM processor - CORRECTED FOR COLAB\"\"\"\n",
        "\n",
        "    # No imports needed - classes are already defined in previous cells\n",
        "\n",
        "    # Initialize components\n",
        "    data_manager = HealthDataManager()\n",
        "    analytics = HealthAnalytics(data_manager)\n",
        "\n",
        "    # Create user profile first\n",
        "    user_id = \"user_123\"\n",
        "    profile = UserProfile(user_id)\n",
        "    profile.update_targets({\n",
        "        'daily_steps': 10000,\n",
        "        'weekly_zone_minutes': 150,\n",
        "        'daily_sleep_hours': 8,\n",
        "        'weekly_exercise_sessions': 3,\n",
        "        'daily_calories': 2000\n",
        "    })\n",
        "    data_manager.save_user_profile(profile)\n",
        "    print(f\"‚úÖ Created user profile for {user_id}\")\n",
        "\n",
        "    # Create some historical data for better analysis\n",
        "    historical_week = WeeklyHealthData(user_id, \"2024-01-22\", {\n",
        "        'total_steps': 62000,\n",
        "        'zone_minutes': 100,\n",
        "        'sleep_hours': [6.8, 6.0, 7.5, 6.5, 6.2, 8.0, 7.2],\n",
        "        'exercise_sessions': [{'type': 'walking', 'duration': 30}],\n",
        "        'mood_scores': [6, 5, 7, 6, 4, 7, 6],\n",
        "        'food_data': []\n",
        "    })\n",
        "    data_manager.save_weekly_data(historical_week)\n",
        "    print(\"‚úÖ Created historical data\")\n",
        "\n",
        "    try:\n",
        "        # Initialize LLM processor with your model paths\n",
        "        llm_processor = HealthLLMProcessor(\n",
        "            base_model_id=\"ContactDoctor/Bio-Medical-Llama-3-8B\",\n",
        "            adapter_path=\"AnjaliNV/WellBeing_LLM\",\n",
        "            vector_db_path=\"/content/drive/MyDrive/rag_index\",  # Your vector DB path\n",
        "            data_manager=data_manager\n",
        "        )\n",
        "        print(\"‚úÖ LLM processor initialized\")\n",
        "\n",
        "        # Current week data\n",
        "        current_week_data = WeeklyHealthData(user_id, \"2024-01-29\", {\n",
        "            'total_steps': 58000,  # Below target\n",
        "            'zone_minutes': 90,    # Below target\n",
        "            'sleep_hours': [6.5, 6.2, 7.8, 6.8, 6.0, 8.2, 7.5],\n",
        "            'exercise_sessions': [\n",
        "                {'type': 'yoga', 'duration': 45}\n",
        "            ],\n",
        "            'mood_scores': [6, 5, 7, 6, 5, 8, 7],\n",
        "            'food_data': []\n",
        "        })\n",
        "        data_manager.save_weekly_data(current_week_data)\n",
        "        print(\"‚úÖ Created current week data\")\n",
        "\n",
        "        # Analyze the week\n",
        "        analysis = analytics.analyze_weekly_performance(user_id, current_week_data)\n",
        "        print(\"‚úÖ Analysis completed\")\n",
        "\n",
        "        # Check for analysis errors\n",
        "        if 'error' in analysis:\n",
        "            print(f\"‚ùå Analysis error: {analysis['error']}\")\n",
        "            return None\n",
        "\n",
        "        # Generate recommendations\n",
        "        print(\"ü§ñ Generating recommendations with LLM...\")\n",
        "        recommendations = llm_processor.generate_weekly_recommendations(user_id, analysis)\n",
        "        print(\"‚úÖ Recommendations generated\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"=== COMPREHENSIVE WEEKLY RECOMMENDATIONS ===\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"üë§ User: {user_id}\")\n",
        "        print(f\"üìÖ Week: {current_week_data.week_start}\")\n",
        "        print(f\"üéØ Overall Score: {analysis['performance']['overall']['score']:.1f}/100\")\n",
        "\n",
        "        print(\"\\nüìä PRIORITY AREAS:\")\n",
        "        if analysis['priorities']:\n",
        "            for priority in analysis['priorities'][:2]:\n",
        "                print(f\"‚Ä¢ {priority['area'].replace('_', ' ').title()}: {', '.join(priority['reasons'])}\")\n",
        "        else:\n",
        "            print(\"‚Ä¢ No major priority areas - maintain current performance\")\n",
        "\n",
        "        print(\"\\nüçé FOOD RECOMMENDATIONS:\")\n",
        "        print(recommendations.get('food_recommendation', 'No specific food recommendations available'))\n",
        "\n",
        "        print(\"\\nüí™ EXERCISE RECOMMENDATIONS:\")\n",
        "        print(recommendations.get('physical_exercise', 'No specific exercise recommendations available'))\n",
        "\n",
        "        print(\"\\nüò¥ SLEEP & RECOVERY:\")\n",
        "        print(recommendations.get('sleep_and_recovery', 'No specific sleep recommendations available'))\n",
        "\n",
        "        print(\"\\nüéØ OVERALL SUGGESTIONS:\")\n",
        "        print(recommendations.get('overall_suggestion', 'No specific overall suggestions available'))\n",
        "\n",
        "        # Save the recommendations\n",
        "        weekly_recs = WeeklyRecommendations(user_id, current_week_data.week_start, {\n",
        "            'food': recommendations.get('food_recommendation', ''),\n",
        "            'exercise': recommendations.get('physical_exercise', ''),\n",
        "            'sleep': recommendations.get('sleep_and_recovery', ''),\n",
        "            'overall': recommendations.get('overall_suggestion', '')\n",
        "        })\n",
        "\n",
        "        data_manager.save_recommendations(weekly_recs)\n",
        "        print(\"\\n‚úÖ Recommendations saved for future learning\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during LLM processing: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Return a fallback recommendation structure\n",
        "        return {\n",
        "            'food_recommendation': 'Error generating recommendations - please try again',\n",
        "            'physical_exercise': 'Error generating recommendations - please try again',\n",
        "            'sleep_and_recovery': 'Error generating recommendations - please try again',\n",
        "            'overall_suggestion': 'Error generating recommendations - please try again',\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "# Test the LLM processing (optional - uncomment to test)\n",
        "# if __name__ == \"__main__\":\n",
        "#     recommendations = example_llm_processing()"
      ],
      "metadata": {
        "id": "XYVeKQzS0TEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================================\n",
        "# STEP 4: Complete Integrated Health Analytics System\n",
        "# ========================================================================\n",
        "import gradio as gr\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "from typing import Dict, List\n",
        "\n",
        "class HealthAnalyticsSystem:\n",
        "    \"\"\"Complete health analytics system with memory and learning\"\"\"\n",
        "\n",
        "    def __init__(self, base_model_id: str, adapter_path: str, vector_db_path: str, data_dir: str = \"health_data\"):\n",
        "        \"\"\"Initialize the complete system\"\"\"\n",
        "\n",
        "        print(\"üöÄ Initializing Health Analytics System...\")\n",
        "\n",
        "        # No imports needed in Colab - classes already defined in previous cells\n",
        "\n",
        "        # Initialize components\n",
        "        self.data_manager = HealthDataManager(data_dir)\n",
        "        self.analytics = HealthAnalytics(self.data_manager)\n",
        "        self.llm_processor = HealthLLMProcessor(base_model_id, adapter_path, vector_db_path, self.data_manager)\n",
        "\n",
        "        # Store classes for creating objects (reference to global classes)\n",
        "        self.UserProfile = UserProfile\n",
        "        self.WeeklyHealthData = WeeklyHealthData\n",
        "        self.WeeklyRecommendations = WeeklyRecommendations\n",
        "\n",
        "        print(\"‚úÖ Health Analytics System ready!\")\n",
        "\n",
        "    def process_weekly_data(self, user_id: str, week_data: Dict, targets: Dict = None) -> Dict:\n",
        "        \"\"\"Main function to process weekly health data and generate recommendations\"\"\"\n",
        "\n",
        "        # Ensure user profile exists\n",
        "        profile = self.data_manager.load_user_profile(user_id)\n",
        "        if not profile:\n",
        "            profile = self.UserProfile(user_id)\n",
        "            if targets:\n",
        "                profile.update_targets(targets)\n",
        "            self.data_manager.save_user_profile(profile)\n",
        "            print(f\"Created new user profile for {user_id}\")\n",
        "        elif targets:\n",
        "            profile.update_targets(targets)\n",
        "            self.data_manager.save_user_profile(profile)\n",
        "\n",
        "        # Create weekly data object\n",
        "        week_start = week_data.get('week_start', datetime.now().strftime('%Y-%m-%d'))\n",
        "        weekly_health_data = self.WeeklyHealthData(user_id, week_start, week_data)\n",
        "\n",
        "        # Save weekly data\n",
        "        self.data_manager.save_weekly_data(weekly_health_data)\n",
        "\n",
        "        # Perform comprehensive analysis\n",
        "        analysis = self.analytics.analyze_weekly_performance(user_id, weekly_health_data)\n",
        "\n",
        "        # Generate recommendations using LLM\n",
        "        recommendations = self.llm_processor.generate_weekly_recommendations(user_id, analysis)\n",
        "\n",
        "        # Save recommendations\n",
        "        weekly_recs = self.WeeklyRecommendations(user_id, week_start, {\n",
        "            'food': recommendations.get('food_recommendation', ''),\n",
        "            'exercise': recommendations.get('physical_exercise', ''),\n",
        "            'sleep': recommendations.get('sleep_and_recovery', ''),\n",
        "            'overall': recommendations.get('overall_suggestion', ''),\n",
        "            'analysis_summary': self._create_analysis_summary(analysis)\n",
        "        })\n",
        "\n",
        "        self.data_manager.save_recommendations(weekly_recs)\n",
        "\n",
        "        # Return comprehensive result\n",
        "        return {\n",
        "            'user_id': user_id,\n",
        "            'week_start': week_start,\n",
        "            'analysis': analysis,\n",
        "            'recommendations': recommendations,\n",
        "            'insights': self.analytics.generate_insights(analysis),\n",
        "            'week_summary': self._create_week_summary(weekly_health_data, analysis)\n",
        "        }\n",
        "\n",
        "    def provide_feedback(self, user_id: str, week_start: str, feedback: Dict):\n",
        "        \"\"\"Allow users to provide feedback on recommendations\"\"\"\n",
        "\n",
        "        self.llm_processor.update_recommendation_feedback(user_id, week_start, feedback)\n",
        "        print(f\"Feedback updated for {user_id}, week {week_start}\")\n",
        "\n",
        "    def get_user_progress(self, user_id: str, weeks: int = 8) -> Dict:\n",
        "        \"\"\"Get comprehensive user progress over time\"\"\"\n",
        "\n",
        "        progress_summary = self.data_manager.get_user_progress_summary(user_id)\n",
        "        recent_recommendations = self.data_manager.load_user_recommendations(user_id, weeks)\n",
        "\n",
        "        return {\n",
        "            'progress_summary': progress_summary,\n",
        "            'recent_recommendations': [rec.to_dict() for rec in recent_recommendations],\n",
        "            'trend_analysis': self._analyze_long_term_trends(user_id, weeks)\n",
        "        }\n",
        "\n",
        "    def _create_analysis_summary(self, analysis: Dict) -> str:\n",
        "        \"\"\"Create a summary of the analysis\"\"\"\n",
        "\n",
        "        performance = analysis.get('performance', {})\n",
        "        overall_score = performance.get('overall', {}).get('score', 0)\n",
        "\n",
        "        summary = f\"Overall Health Score: {overall_score:.1f}/100\\n\"\n",
        "\n",
        "        # Performance breakdown\n",
        "        summary += \"\\nPerformance Breakdown:\\n\"\n",
        "        for area, perf in performance.items():\n",
        "            if area != 'overall':\n",
        "                summary += f\"‚Ä¢ {area.replace('_', ' ').title()}: {perf.get('achievement_percent', 0):.1f}%\\n\"\n",
        "\n",
        "        # Priority areas\n",
        "        priorities = analysis.get('priorities', [])\n",
        "        if priorities:\n",
        "            summary += f\"\\nTop Priority: {priorities[0]['area'].replace('_', ' ').title()}\\n\"\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def _create_week_summary(self, weekly_data: WeeklyHealthData, analysis: Dict) -> str:\n",
        "        \"\"\"Create a human-readable week summary\"\"\"\n",
        "\n",
        "        summary = f\"Week of {weekly_data.week_start}:\\n\"\n",
        "        summary += f\"‚Ä¢ Steps: {weekly_data.total_steps:,} ({weekly_data.avg_daily_steps:.0f}/day)\\n\"\n",
        "        summary += f\"‚Ä¢ Zone Minutes: {weekly_data.zone_minutes}\\n\"\n",
        "        summary += f\"‚Ä¢ Average Sleep: {weekly_data.avg_sleep:.1f} hours\\n\"\n",
        "        summary += f\"‚Ä¢ Exercise Sessions: {len(weekly_data.exercise_sessions)}\\n\"\n",
        "        summary += f\"‚Ä¢ Average Mood: {weekly_data.avg_mood:.1f}/10\\n\"\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def _analyze_long_term_trends(self, user_id: str, weeks: int) -> Dict:\n",
        "        \"\"\"Analyze long-term trends\"\"\"\n",
        "\n",
        "        weekly_data = self.data_manager.load_user_weekly_data(user_id, weeks)\n",
        "\n",
        "        if len(weekly_data) < 3:\n",
        "            return {\"message\": \"Insufficient data for trend analysis\"}\n",
        "\n",
        "        # Calculate trends\n",
        "        weeks_data = list(reversed(weekly_data))  # Chronological order\n",
        "\n",
        "        trends = {\n",
        "            'steps_trend': self._calculate_simple_trend([w.avg_daily_steps for w in weeks_data]),\n",
        "            'zone_trend': self._calculate_simple_trend([w.zone_minutes for w in weeks_data]),\n",
        "            'sleep_trend': self._calculate_simple_trend([w.avg_sleep for w in weeks_data]),\n",
        "            'mood_trend': self._calculate_simple_trend([w.avg_mood for w in weeks_data if w.avg_mood > 0])\n",
        "        }\n",
        "\n",
        "        return trends\n",
        "\n",
        "    def _calculate_simple_trend(self, data: List[float]) -> str:\n",
        "        \"\"\"Calculate simple trend direction\"\"\"\n",
        "        if len(data) < 3:\n",
        "            return \"insufficient_data\"\n",
        "\n",
        "        recent_avg = sum(data[-3:]) / 3\n",
        "        earlier_avg = sum(data[:3]) / 3\n",
        "\n",
        "        change_percent = ((recent_avg - earlier_avg) / earlier_avg) * 100 if earlier_avg > 0 else 0\n",
        "\n",
        "        if change_percent > 5:\n",
        "            return \"improving\"\n",
        "        elif change_percent < -5:\n",
        "            return \"declining\"\n",
        "        else:\n",
        "            return \"stable\"\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 4: Enhanced Gradio Interface\n",
        "# ========================================================================\n",
        "\n",
        "class HealthAnalyticsInterface:\n",
        "    \"\"\"Enhanced Gradio interface for the health analytics system\"\"\"\n",
        "\n",
        "    def __init__(self, health_system: HealthAnalyticsSystem):\n",
        "        self.health_system = health_system\n",
        "        self.current_user_id = \"default_user\"\n",
        "\n",
        "    def create_interface(self):\n",
        "        \"\"\"Create the Gradio interface\"\"\"\n",
        "\n",
        "        with gr.Blocks(title=\"Health Analytics AI\", theme=gr.themes.Soft()) as demo:\n",
        "            gr.Markdown(\"# üè• Health Analytics AI Assistant\")\n",
        "            gr.Markdown(\"Analyze your weekly health data and get personalized recommendations\")\n",
        "\n",
        "            with gr.Tab(\"üìä Weekly Analysis\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        user_id_input = gr.Textbox(\n",
        "                            label=\"User ID\",\n",
        "                            value=\"user_123\",\n",
        "                            placeholder=\"Enter your user ID\"\n",
        "                        )\n",
        "\n",
        "                        week_start_input = gr.Textbox(\n",
        "                            label=\"Week Start Date\",\n",
        "                            value=datetime.now().strftime('%Y-%m-%d'),\n",
        "                            placeholder=\"YYYY-MM-DD\"\n",
        "                        )\n",
        "\n",
        "                        with gr.Row():\n",
        "                            steps_input = gr.Number(\n",
        "                                label=\"Total Steps (Week)\",\n",
        "                                value=65000,\n",
        "                                minimum=0\n",
        "                            )\n",
        "                            zone_minutes_input = gr.Number(\n",
        "                                label=\"Zone Minutes (Week)\",\n",
        "                                value=120,\n",
        "                                minimum=0\n",
        "                            )\n",
        "\n",
        "                        sleep_input = gr.Textbox(\n",
        "                            label=\"Daily Sleep Hours\",\n",
        "                            value=\"7.5,6.8,8.2,7.0,6.5,8.5,7.8\",\n",
        "                            placeholder=\"Enter 7 values separated by commas\"\n",
        "                        )\n",
        "\n",
        "                        mood_input = gr.Textbox(\n",
        "                            label=\"Daily Mood Scores (1-10)\",\n",
        "                            value=\"7,6,8,7,5,8,7\",\n",
        "                            placeholder=\"Enter 7 values separated by commas\"\n",
        "                        )\n",
        "\n",
        "                        exercise_input = gr.Number(\n",
        "                            label=\"Exercise Sessions\",\n",
        "                            value=2,\n",
        "                            minimum=0\n",
        "                        )\n",
        "\n",
        "                        calories_input = gr.Number(\n",
        "                            label=\"Total Calories (Week)\",\n",
        "                            value=14000,\n",
        "                            minimum=0\n",
        "                        )\n",
        "\n",
        "                        analyze_btn = gr.Button(\"üîç Analyze Health Data\", variant=\"primary\")\n",
        "\n",
        "                    with gr.Column():\n",
        "                        analysis_output = gr.Markdown(\"Click 'Analyze Health Data' to see your personalized recommendations\")\n",
        "\n",
        "            with gr.Tab(\"üìà Progress Tracking\"):\n",
        "                with gr.Row():\n",
        "                    progress_user_input = gr.Textbox(\n",
        "                        label=\"User ID\",\n",
        "                        value=\"user_123\"\n",
        "                    )\n",
        "                    weeks_input = gr.Number(\n",
        "                        label=\"Number of Weeks\",\n",
        "                        value=4,\n",
        "                        minimum=1,\n",
        "                        maximum=12\n",
        "                    )\n",
        "                    progress_btn = gr.Button(\"üìä View Progress\")\n",
        "\n",
        "                progress_output = gr.Markdown(\"Enter user ID and click 'View Progress'\")\n",
        "\n",
        "            with gr.Tab(\"üí¨ Feedback\"):\n",
        "                with gr.Row():\n",
        "                    feedback_user_input = gr.Textbox(\n",
        "                        label=\"User ID\",\n",
        "                        value=\"user_123\"\n",
        "                    )\n",
        "                    feedback_week_input = gr.Textbox(\n",
        "                        label=\"Week Date\",\n",
        "                        value=datetime.now().strftime('%Y-%m-%d')\n",
        "                    )\n",
        "\n",
        "                with gr.Row():\n",
        "                    followed_exercise = gr.Checkbox(label=\"Followed Exercise Recommendations\")\n",
        "                    followed_nutrition = gr.Checkbox(label=\"Followed Nutrition Recommendations\")\n",
        "                    followed_sleep = gr.Checkbox(label=\"Followed Sleep Recommendations\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    difficulty_rating = gr.Slider(\n",
        "                        label=\"Difficulty Level (1-5)\",\n",
        "                        minimum=1,\n",
        "                        maximum=5,\n",
        "                        value=3\n",
        "                    )\n",
        "                    effectiveness_rating = gr.Slider(\n",
        "                        label=\"Effectiveness (1-5)\",\n",
        "                        minimum=1,\n",
        "                        maximum=5,\n",
        "                        value=3\n",
        "                    )\n",
        "\n",
        "                feedback_notes = gr.Textbox(\n",
        "                    label=\"Additional Notes\",\n",
        "                    placeholder=\"Any additional feedback about the recommendations...\"\n",
        "                )\n",
        "\n",
        "                feedback_btn = gr.Button(\"üíå Submit Feedback\")\n",
        "                feedback_output = gr.Markdown(\"Submit your feedback to help improve future recommendations\")\n",
        "\n",
        "            # Event handlers\n",
        "            analyze_btn.click(\n",
        "                fn=self.analyze_weekly_data,\n",
        "                inputs=[\n",
        "                    user_id_input, week_start_input, steps_input, zone_minutes_input,\n",
        "                    sleep_input, mood_input, exercise_input, calories_input\n",
        "                ],\n",
        "                outputs=[analysis_output]\n",
        "            )\n",
        "\n",
        "            progress_btn.click(\n",
        "                fn=self.view_progress,\n",
        "                inputs=[progress_user_input, weeks_input],\n",
        "                outputs=[progress_output]\n",
        "            )\n",
        "\n",
        "            feedback_btn.click(\n",
        "                fn=self.submit_feedback,\n",
        "                inputs=[\n",
        "                    feedback_user_input, feedback_week_input, followed_exercise,\n",
        "                    followed_nutrition, followed_sleep, difficulty_rating,\n",
        "                    effectiveness_rating, feedback_notes\n",
        "                ],\n",
        "                outputs=[feedback_output]\n",
        "            )\n",
        "\n",
        "        return demo\n",
        "\n",
        "    def analyze_weekly_data(self, user_id, week_start, steps, zone_minutes, sleep_str, mood_str, exercise_sessions, calories):\n",
        "        \"\"\"Process weekly data and return analysis\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Parse sleep and mood data\n",
        "            sleep_hours = [float(x.strip()) for x in sleep_str.split(',') if x.strip()]\n",
        "            mood_scores = [float(x.strip()) for x in mood_str.split(',') if x.strip()]\n",
        "\n",
        "            # Create week data\n",
        "            week_data = {\n",
        "                'week_start': week_start,\n",
        "                'total_steps': int(steps),\n",
        "                'zone_minutes': int(zone_minutes),\n",
        "                'sleep_hours': sleep_hours,\n",
        "                'mood_scores': mood_scores,\n",
        "                'exercise_sessions': [{'type': 'general', 'duration': 30} for _ in range(int(exercise_sessions))],\n",
        "                'food_data': [{'calories': calories / 7} for _ in range(7)]  # Distribute calories\n",
        "            }\n",
        "\n",
        "            # Default targets\n",
        "            targets = {\n",
        "                'daily_steps': 10000,\n",
        "                'weekly_zone_minutes': 150,\n",
        "                'daily_sleep_hours': 8,\n",
        "                'weekly_exercise_sessions': 3,\n",
        "                'daily_calories': 2000\n",
        "            }\n",
        "\n",
        "            # Process the data\n",
        "            result = self.health_system.process_weekly_data(user_id, week_data, targets)\n",
        "\n",
        "            # Format the output\n",
        "            return self._format_analysis_output(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error processing data: {str(e)}\\n\\nPlease check your input format.\"\n",
        "\n",
        "    def view_progress(self, user_id, weeks):\n",
        "        \"\"\"View user progress over time\"\"\"\n",
        "\n",
        "        try:\n",
        "            progress = self.health_system.get_user_progress(user_id, int(weeks))\n",
        "            return self._format_progress_output(progress)\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error retrieving progress: {str(e)}\"\n",
        "\n",
        "    def submit_feedback(self, user_id, week_date, followed_exercise, followed_nutrition, followed_sleep, difficulty, effectiveness, notes):\n",
        "        \"\"\"Submit feedback on recommendations\"\"\"\n",
        "\n",
        "        try:\n",
        "            feedback = {\n",
        "                'followed_exercise': followed_exercise,\n",
        "                'followed_nutrition': followed_nutrition,\n",
        "                'followed_sleep': followed_sleep,\n",
        "                'difficulty_level': difficulty,\n",
        "                'effectiveness': effectiveness,\n",
        "                'notes': notes\n",
        "            }\n",
        "\n",
        "            self.health_system.provide_feedback(user_id, week_date, feedback)\n",
        "\n",
        "            return f\"\"\"‚úÖ **Feedback Submitted Successfully!**\n",
        "\n",
        "**Week:** {week_date}\n",
        "**Followed Recommendations:**\n",
        "- Exercise: {'‚úÖ' if followed_exercise else '‚ùå'}\n",
        "- Nutrition: {'‚úÖ' if followed_nutrition else '‚ùå'}\n",
        "- Sleep: {'‚úÖ' if followed_sleep else '‚ùå'}\n",
        "\n",
        "**Ratings:**\n",
        "- Difficulty: {difficulty}/5\n",
        "- Effectiveness: {effectiveness}/5\n",
        "\n",
        "**Notes:** {notes if notes else 'None'}\n",
        "\n",
        "Your feedback helps improve future recommendations!\"\"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error submitting feedback: {str(e)}\"\n",
        "\n",
        "    def _format_analysis_output(self, result: Dict) -> str:\n",
        "        \"\"\"Format the analysis output for display\"\"\"\n",
        "\n",
        "        analysis = result['analysis']\n",
        "        recommendations = result['recommendations']\n",
        "        insights = result['insights']\n",
        "\n",
        "        output = f\"\"\"# üè• Health Analysis for {result['user_id']}\n",
        "\n",
        "## üìä **Week Summary** ({result['week_start']})\n",
        "{result['week_summary']}\n",
        "\n",
        "## üéØ **Overall Health Score: {analysis['performance']['overall']['score']:.1f}/100**\n",
        "\n",
        "## üìà **Key Insights**\n",
        "\"\"\"\n",
        "\n",
        "        for insight in insights:\n",
        "            output += f\"- {insight}\\n\"\n",
        "\n",
        "        output += \"\\n## üèÜ **Performance Breakdown**\\n\"\n",
        "        for area, perf in analysis['performance'].items():\n",
        "            if area != 'overall':\n",
        "                status_emoji = {'excellent': 'üü¢', 'good': 'üü°', 'fair': 'üü†', 'needs_improvement': 'üî¥'}.get(perf['status'], '‚ö™')\n",
        "                output += f\"- **{area.replace('_', ' ').title()}:** {perf['achievement_percent']:.1f}% {status_emoji}\\n\"\n",
        "\n",
        "        if analysis['priorities']:\n",
        "            output += \"\\n## ‚ö†Ô∏è **Priority Areas**\\n\"\n",
        "            for priority in analysis['priorities'][:3]:\n",
        "                output += f\"- **{priority['area'].replace('_', ' ').title()}:** {', '.join(priority['reasons'])}\\n\"\n",
        "\n",
        "        output += f\"\\n## üçé **Food Recommendations**\\n{recommendations.get('food_recommendation', 'No specific recommendations')}\\n\"\n",
        "        output += f\"\\n## üí™ **Exercise Recommendations**\\n{recommendations.get('physical_exercise', 'No specific recommendations')}\\n\"\n",
        "        output += f\"\\n## üò¥ **Sleep & Recovery**\\n{recommendations.get('sleep_and_recovery', 'No specific recommendations')}\\n\"\n",
        "        output += f\"\\n## üéØ **Overall Suggestions**\\n{recommendations.get('overall_suggestion', 'No specific recommendations')}\\n\"\n",
        "\n",
        "        output += \"\\n---\\n*üí° Tip: Use the Feedback tab to let us know how these recommendations work for you!*\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _format_progress_output(self, progress: Dict) -> str:\n",
        "        \"\"\"Format progress output for display\"\"\"\n",
        "\n",
        "        summary = progress.get('progress_summary', {})\n",
        "        trends = progress.get('trend_analysis', {})\n",
        "\n",
        "        if summary.get('message'):\n",
        "            return f\"‚ÑπÔ∏è {summary['message']}\"\n",
        "\n",
        "        output = f\"\"\"# üìà Progress Report\n",
        "\n",
        "## üìä **Tracking Summary**\n",
        "- **Weeks Tracked:** {summary.get('weeks_tracked', 0)}\n",
        "- **Data Points Available:** {len(summary.get('steps_trend', []))}\n",
        "\n",
        "## üìà **Long-term Trends**\n",
        "\"\"\"\n",
        "\n",
        "        trend_emojis = {'improving': 'üìà', 'declining': 'üìâ', 'stable': '‚û°Ô∏è', 'insufficient_data': '‚ùì'}\n",
        "\n",
        "        for area, trend in trends.items():\n",
        "            emoji = trend_emojis.get(trend, '‚ùì')\n",
        "            output += f\"- **{area.replace('_', ' ').title()}:** {trend.replace('_', ' ').title()} {emoji}\\n\"\n",
        "\n",
        "        if summary.get('latest_week'):\n",
        "            latest = summary['latest_week']\n",
        "            output += f\"\"\"\n",
        "## üìÖ **Latest Week Data**\n",
        "- **Week:** {latest.get('week_start')}\n",
        "- **Daily Steps:** {latest.get('avg_daily_steps', 0):.0f}\n",
        "- **Zone Minutes:** {latest.get('zone_minutes', 0)}\n",
        "- **Sleep:** {latest.get('avg_sleep', 0):.1f} hours\n",
        "- **Mood:** {latest.get('avg_mood', 0):.1f}/10\n",
        "\"\"\"\n",
        "\n",
        "        # Recent recommendations summary\n",
        "        recent_recs = progress.get('recent_recommendations', [])\n",
        "        if recent_recs:\n",
        "            output += f\"\\n## üìù **Recent Recommendations** (Last {len(recent_recs)} weeks)\\n\"\n",
        "            for i, rec in enumerate(recent_recs[:3], 1):\n",
        "                output += f\"{i}. **Week {rec['week_start']}:** {rec['recommendations'].get('overall', 'No summary available')[:100]}...\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 4: Main Application\n",
        "# ========================================================================\n",
        "\n",
        "def create_health_analytics_app():\n",
        "    \"\"\"Create and launch the complete health analytics application\"\"\"\n",
        "\n",
        "    # Initialize the health analytics system\n",
        "    health_system = HealthAnalyticsSystem(\n",
        "        base_model_id=\"ContactDoctor/Bio-Medical-Llama-3-8B\",\n",
        "        adapter_path=\"AnjaliNV/WellBeing_LLM\",\n",
        "        vector_db_path=\"/content/drive/MyDrive/rag_index\",  # Your vector DB path\n",
        "        data_dir=\"health_data\"\n",
        "    )\n",
        "\n",
        "    # Create the interface\n",
        "    interface = HealthAnalyticsInterface(health_system)\n",
        "    demo = interface.create_interface()\n",
        "\n",
        "    return demo, health_system\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 4: Usage Example with Sample Data\n",
        "# ========================================================================\n",
        "\n",
        "def run_example_analysis():\n",
        "    \"\"\"Run example analysis to demonstrate the system - CORRECTED FOR COLAB\"\"\"\n",
        "\n",
        "    print(\"üß™ Running example analysis...\")\n",
        "\n",
        "    # Create the system\n",
        "    demo, health_system = create_health_analytics_app()\n",
        "\n",
        "    # Example: Simulate 3 weeks of data for a user\n",
        "    user_id = \"demo_user\"\n",
        "\n",
        "    # Week 1 - Baseline\n",
        "    week1_data = {\n",
        "        'week_start': '2024-01-15',\n",
        "        'total_steps': 58000,\n",
        "        'zone_minutes': 90,\n",
        "        'sleep_hours': [6.5, 6.2, 7.8, 6.8, 6.0, 8.2, 7.5],\n",
        "        'exercise_sessions': [{'type': 'yoga', 'duration': 45}],\n",
        "        'mood_scores': [6, 5, 7, 6, 5, 8, 7],\n",
        "        'food_data': [{'calories': 400} for _ in range(21)]  # 3 meals/day\n",
        "    }\n",
        "\n",
        "    targets = {\n",
        "        'daily_steps': 10000,\n",
        "        'weekly_zone_minutes': 150,\n",
        "        'daily_sleep_hours': 8,\n",
        "        'weekly_exercise_sessions': 3,\n",
        "        'daily_calories': 2000\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        result1 = health_system.process_weekly_data(user_id, week1_data, targets)\n",
        "        print(f\"‚úÖ Week 1 processed - Score: {result1['analysis']['performance']['overall']['score']:.1f}/100\")\n",
        "\n",
        "        # Week 2 - Some improvement\n",
        "        week2_data = {\n",
        "            'week_start': '2024-01-22',\n",
        "            'total_steps': 68000,  # Improved\n",
        "            'zone_minutes': 130,   # Improved\n",
        "            'sleep_hours': [7.2, 6.8, 8.1, 7.3, 6.8, 8.0, 7.6],\n",
        "            'exercise_sessions': [\n",
        "                {'type': 'running', 'duration': 30},\n",
        "                {'type': 'yoga', 'duration': 45}\n",
        "            ],\n",
        "            'mood_scores': [7, 6, 8, 7, 6, 8, 7],\n",
        "            'food_data': [{'calories': 400} for _ in range(21)]\n",
        "        }\n",
        "\n",
        "        result2 = health_system.process_weekly_data(user_id, week2_data)\n",
        "        print(f\"‚úÖ Week 2 processed - Score: {result2['analysis']['performance']['overall']['score']:.1f}/100\")\n",
        "\n",
        "        # Week 3 - Further improvement\n",
        "        week3_data = {\n",
        "            'week_start': '2024-01-29',\n",
        "            'total_steps': 74000,  # Further improved\n",
        "            'zone_minutes': 155,   # Target reached!\n",
        "            'sleep_hours': [7.8, 7.5, 8.2, 7.7, 7.2, 8.1, 7.9],\n",
        "            'exercise_sessions': [\n",
        "                {'type': 'running', 'duration': 35},\n",
        "                {'type': 'cycling', 'duration': 40},\n",
        "                {'type': 'yoga', 'duration': 45}\n",
        "            ],\n",
        "            'mood_scores': [8, 7, 8, 8, 7, 9, 8],\n",
        "            'food_data': [{'calories': 400} for _ in range(21)]\n",
        "        }\n",
        "\n",
        "        result3 = health_system.process_weekly_data(user_id, week3_data)\n",
        "        print(f\"‚úÖ Week 3 processed - Score: {result3['analysis']['performance']['overall']['score']:.1f}/100\")\n",
        "\n",
        "        # Show progress\n",
        "        progress = health_system.get_user_progress(user_id, 3)\n",
        "        print(f\"\\nüìà Progress Summary:\")\n",
        "        print(f\"- Weeks tracked: {progress['progress_summary']['weeks_tracked']}\")\n",
        "        print(f\"- Steps trend: {progress['trend_analysis'].get('steps_trend', 'N/A')}\")\n",
        "        print(f\"- Zone minutes trend: {progress['trend_analysis'].get('zone_trend', 'N/A')}\")\n",
        "        print(f\"- Sleep trend: {progress['trend_analysis'].get('sleep_trend', 'N/A')}\")\n",
        "\n",
        "        # Simulate feedback\n",
        "        health_system.provide_feedback(user_id, '2024-01-22', {\n",
        "            'followed_exercise': True,\n",
        "            'followed_nutrition': True,\n",
        "            'followed_sleep': False,\n",
        "            'difficulty_level': 3,\n",
        "            'effectiveness': 4,\n",
        "            'notes': 'Exercise recommendations were great! Sleep tips were hard to follow.'\n",
        "        })\n",
        "        print(\"‚úÖ Feedback submitted\")\n",
        "\n",
        "        print(\"\\nüöÄ System ready! Use demo.launch() to start the interface.\")\n",
        "        return demo, health_system\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during example analysis: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return demo, health_system\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the example\n",
        "    demo, health_system = run_example_analysis()\n",
        "\n",
        "    # Launch the interface\n",
        "    # demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QdjMVziJ0TzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "id": "43mfkcET06HY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "https://github.com/AnjaliVaghjiani/Thesis/blob/main/ContactDoctor.ipynb",
      "authorship_tag": "ABX9TyPT3BAR/hweMgLNlugqNkSW"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}