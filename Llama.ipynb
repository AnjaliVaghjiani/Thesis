{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpYvg_ZSHJ45"
      },
      "outputs": [],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrYRwDFP3VnL"
      },
      "source": [
        "# MedALpaca LLM\n",
        "\n",
        "- required GPU RAM > 15GB so\n",
        "- Device used L4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ2IZnY0spOf"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHa4OWkuvz3S"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNVvxTdtlN3a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import gradio as gr\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_id = \"medalpaca/medalpaca-7b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n",
        "\n",
        "# Chat function with history\n",
        "def chat_interface(message, history):\n",
        "    # Reconstruct prompt with history\n",
        "    prompt = \"\"\n",
        "    for user_msg, bot_msg in history:\n",
        "        prompt += f\"### Instruction:\\n{user_msg}\\n\\n### Response:\\n{bot_msg}\\n\\n\"\n",
        "    prompt += f\"### Instruction:\\n{message}\\n\\n### Response:\\n\"\n",
        "\n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Only return new response (strip old prompt)\n",
        "    if \"### Response:\" in response:\n",
        "        response = response.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Launch Gradio Chat Interface\n",
        "gr.ChatInterface(\n",
        "    fn=chat_interface,\n",
        "    title=\"ðŸ§  MedAlpaca 7B - Medical Chatbot\",\n",
        "    description=\"Ask medical or wellness-related questions. Powered by medAlpaca 7B.\",\n",
        ").launch(debug=True, share=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQs-1g9pO2vc"
      },
      "source": [
        "# ContactDoctor Bio Medical LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zBB_GqePIUt"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes gradio flash_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFQNul7TO1AW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# ---- Model Setup ----\n",
        "model_id = \"ContactDoctor/Bio-Medical-Llama-3-8B\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "                                bnb_4bit_use_double_quant=True,\n",
        "                                bnb_4bit_compute_dtype=torch.float16, )\n",
        "\n",
        "print(\"ðŸ”„ Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "print(\"ðŸ”„ Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# ---- Chat Function ----\n",
        "def chat_only_interface(message, history):\n",
        "\n",
        "#the tempelet to receive the prompts\n",
        "    prompt_template = f\"\"\"\n",
        "  You are a wellbeing adviser. Respond to the user's condition using the following format:\n",
        "\n",
        "  1. Food Recommendation:\n",
        "  2. Physical Exercise:\n",
        "  3. Social Wellbeing Recommendation:\n",
        "  4. Overall Suggestion:\n",
        "\n",
        "  User Input: \"{message}\"\n",
        "  \"\"\"\n",
        "    inputs = tokenizer(prompt_template, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.95,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Trim to just the model's response part\n",
        "    generated_text = response.split(\"User Input:\")[1] if \"User Input:\" in response else response\n",
        "\n",
        "    with open(\"chat_history.txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"User: {message}\\n\")\n",
        "        f.write(f\"Model: {response}\\n\\n\")\n",
        "\n",
        "    return generated_text.strip()\n",
        "\n",
        "# ---- Chat Interface ----\n",
        "gr.ChatInterface(\n",
        "    fn=chat_only_interface,\n",
        "    title=\"ðŸ§  Bio-Medical LLaMA 3 Chat\",\n",
        "    description=\"Ask wellness or medical-related questions.\",\n",
        ").queue().launch(debug=True, share=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoOA1Lq6E7tM"
      },
      "source": [
        "# Medgemma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwqhvruEFe4U"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tqih35eFbDP"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GK7157J6E_Nb"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "# ---- Model Setup ----\n",
        "model_id = \"google/medgemma-4b-it\"\n",
        "\n",
        "print(\"ðŸ”„ Loading pipeline...\")\n",
        "chatbot = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"google/medgemma-4b-it\",\n",
        "    torch_dtype=\"float32\",  # Change this from float16\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "# ---- Chat Function ----\n",
        "def chat_interface(message, history):\n",
        "    response = chatbot(\n",
        "    message,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,  # Lower = more stable\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1,\n",
        "    do_sample=True,\n",
        ")[0][\"generated_text\"]\n",
        "\n",
        "    # Save history (optional)\n",
        "    with open(\"chat_history_medgemma_pipeline.txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"User: {message}\\n\")\n",
        "        f.write(f\"Model: {response}\\n\\n\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# ---- Gradio Chat Interface ----\n",
        "gr.ChatInterface(\n",
        "    fn=chat_interface,\n",
        "    title=\"ðŸ§  MedGemma 27B - Medical Chatbot\",\n",
        "    description=\"Ask medical or wellness-related questions.\",\n",
        ").queue().launch(debug=True, share=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDhzioJo0ZtR"
      },
      "source": [
        "# DeepSeek Medical Reasoning\n",
        "- Device used to run the mode L4GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS9EKzHc1V6R"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8ZfCxWU1MZS"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets accelerate peft trl bitsandbytes\n",
        "!pip install -U transformers==4.52.1\n",
        "!pip install huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MToxxAqK2M-L"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2NbzpbKs09qN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import gradio as gr\n",
        "\n",
        "# Load model & tokenizer\n",
        "model_id = \"kingabzpro/DeepSeek-R1-0528-Qwen3-8B-Medical-Reasoning\"\n",
        "\n",
        "print(\"ðŸ”„ Loading tokenizer and model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,  # or torch.float32 if you get NaN/Inf errors\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Chat function\n",
        "def chat_interface(message, history):\n",
        "    prompt = message\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Optional: save chat history\n",
        "    with open(\"chat_history_deepseek.txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"User: {message}\\nModel: {response}\\n\\n\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# Gradio interface\n",
        "gr.ChatInterface(\n",
        "    fn=chat_interface,\n",
        "    title=\"ðŸ§  DeepSeek Qwen3 8B - Medical Reasoning Chatbot\",\n",
        "    description=\"Ask medical reasoning or clinical diagnostic questions.\"\n",
        ").queue().launch(debug=True, share=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilXD_-69cJ15"
      },
      "source": [
        "# BioMistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0GyrD8WctMe"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate gradio torch\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mi9tf-2cI-m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import gradio as gr\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_id = \"BioMistral/BioMistral-7B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n",
        "\n",
        "# Chat function with history\n",
        "def chat_interface(message, history):\n",
        "    # Format prompt with history\n",
        "    prompt = \"\"\n",
        "    for user_input, bot_output in history:\n",
        "        prompt += f\"<s>[INST] {user_input} [/INST] {bot_output} </s>\\n\"\n",
        "    prompt += f\"<s>[INST] {message} [/INST]\"\n",
        "\n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.1,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract latest response\n",
        "    if \"[/INST]\" in decoded_output:\n",
        "        response = decoded_output.split(\"[/INST]\")[-1].strip()\n",
        "    else:\n",
        "        response = decoded_output.strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Gradio chat app\n",
        "gr.ChatInterface(\n",
        "    fn=chat_interface,\n",
        "    title=\"ðŸ§¬ BioMistral 7B - Biomedical Chatbot\",\n",
        "    description=\"Ask medical and biomedical questions. Powered by BioMistral-7B.\",\n",
        ").launch(debug=True, share=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "160L8ZkqbBQac84dKxP0_dlSfONCvFDHC",
      "authorship_tag": "ABX9TyMJ2MV7aEjY1ybmlHWSPVQU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}