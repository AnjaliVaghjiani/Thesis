{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ2IZnY0spOf"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQs-1g9pO2vc"
      },
      "source": [
        "# ContactDoctor Bio Medical LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zBB_GqePIUt"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFQNul7TO1AW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# ---- Model Setup ----\n",
        "model_id = \"ContactDoctor/Bio-Medical-Llama-3-8B\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "                                bnb_4bit_use_double_quant=True,\n",
        "                                bnb_4bit_compute_dtype=torch.float16, )\n",
        "\n",
        "print(\"ðŸ”„ Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "print(\"ðŸ”„ Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# ---- Chat Function ----\n",
        "def chat_only_interface(message, history):\n",
        "\n",
        "#the tempelet to receive the prompts\n",
        "    prompt_template = f\"\"\"\n",
        "  You are a wellbeing adviser. Respond to the user's condition using the following format:\n",
        "\n",
        "  1. Food Recommendation:\n",
        "  2. Physical Exercise:\n",
        "  3. Social Wellbeing Recommendation:\n",
        "  4. Overall Suggestion:\n",
        "\n",
        "  User Input: \"{message}\"\n",
        "  \"\"\"\n",
        "    inputs = tokenizer(prompt_template, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.95,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Trim to just the model's response part\n",
        "    generated_text = response.split(\"User Input:\")[1] if \"User Input:\" in response else response\n",
        "\n",
        "    with open(\"chat_history.txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"User: {message}\\n\")\n",
        "        f.write(f\"Model: {response}\\n\\n\")\n",
        "\n",
        "    return generated_text.strip()\n",
        "\n",
        "# ---- Chat Interface ----\n",
        "gr.ChatInterface(\n",
        "    fn=chat_only_interface,\n",
        "    title=\"ðŸ§  Bio-Medical LLaMA 3 Chat\",\n",
        "    description=\"Ask wellness or medical-related questions.\",\n",
        ").queue().launch(debug=True, share=True)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "160L8ZkqbBQac84dKxP0_dlSfONCvFDHC",
      "authorship_tag": "ABX9TyMTEE7r+XxWZ9CBa0QPe//k"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}